{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Retraction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retract = pd.read_csv('./total_retraction_data.csv', index_col=False)\n",
    "retract.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the \"Data Collection\" notebook, the querying script would have ideally run through the entire DOI column. However, I often had 404 errors or the script would stop abruptly with no error message. For this reason, I decided to break the DOI column into small sections to closely monitor the information and save the information I received frequently. Because of this, several CSV files were created, each labeled with what DOI index values were used when pulling the information. This information was then cleaned into one complete CSV file in a notebook that will not be provided. This CSV file is labeled as \"total_retraction_data.csv\" and was opened above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "retract = retract.drop(columns=['Unnamed: 0', 'doi_check'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retract = retract.dropna(axis=0, subset=['text'])\n",
    "print(retract.shape)\n",
    "retract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retract['retraction_binary'] = 1\n",
    "print(retract['retraction_binary'].value_counts())\n",
    "retract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retract.to_csv('./no_null_text_retraction_data.csv')\n",
    "retract = pd.read_csv('./no_null_text_retraction_data.csv')\n",
    "retract = retract.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe was cleaned in several ways. Unnecessary columns were dropped. Any articles that had no text data were dropped. A new column named \"retraction_binary\" was created that was filled with the integer 1 to indicate that all of the articles in this dataframe were articles that had been retracted. Using .info method below, it is evident that the only remaining articles had full text. Once the data was cleaned, it was saved into a new CSV file for proofing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1553 entries, 0 to 1552\n",
      "Data columns (total 16 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   id                 1553 non-null   int64  \n",
      " 1   doi                1553 non-null   object \n",
      " 2   language           1553 non-null   object \n",
      " 3   year               1315 non-null   float64\n",
      " 4   month              1315 non-null   float64\n",
      " 5   day                1315 non-null   float64\n",
      " 6   volume             1518 non-null   float64\n",
      " 7   issue              1043 non-null   float64\n",
      " 8   journal            1553 non-null   object \n",
      " 9   title              1553 non-null   object \n",
      " 10  page               1487 non-null   object \n",
      " 11  text               1553 non-null   object \n",
      " 12  abstract           1533 non-null   object \n",
      " 13  keywords           1553 non-null   object \n",
      " 14  publisher          1549 non-null   object \n",
      " 15  retraction_binary  1553 non-null   int64  \n",
      "dtypes: float64(5), int64(2), object(9)\n",
      "memory usage: 194.2+ KB\n"
     ]
    }
   ],
   "source": [
    "retract.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_keywords_list = []\n",
    "count = 0\n",
    "for i in retract['keywords']:\n",
    "    keywords_list = []\n",
    "    if i == []:\n",
    "        ls_keywords_list.append([])\n",
    "    else:\n",
    "        for j in i.split():\n",
    "            keywords_list.append(j.replace(\"'\",'').replace('[','').replace(',','').replace(']','').replace('(','').replace(')','').replace('\\\\n', '').replace('\\\\n','').lower())\n",
    "        if keywords_list == ['']:\n",
    "            ls_keywords_list.append([])\n",
    "        else:\n",
    "            ls_keywords_list.append(keywords_list)\n",
    "\n",
    "retract['unpacked_keywords'] = ls_keywords_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keywords list for each article was a string. In this format, this information is not useful. Because of this, the above script was used to \"unpack\" each keyword and place it into a true list. These lists were then placed into a new column, \"unpacked_keywords\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "clean_text = []\n",
    "clean_text_lem = []\n",
    "\n",
    "for i in range(0, len(retract['text'])):\n",
    "    ls_words = []\n",
    "    ls_lem = []\n",
    "    for j in tokenizer.tokenize(retract['text'][i]):\n",
    "        try:\n",
    "            int(j)\n",
    "        except:\n",
    "            if len(j) < 45:\n",
    "                ls_words.append(j)\n",
    "                ls_lem.append(lemmatizer.lemmatize(j))\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "    clean_text.append(' '.join(ls_words))\n",
    "    clean_text_lem.append(' '.join(ls_lem))\n",
    "    \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>doi</th>\n",
       "      <th>language</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>volume</th>\n",
       "      <th>issue</th>\n",
       "      <th>journal</th>\n",
       "      <th>title</th>\n",
       "      <th>page</th>\n",
       "      <th>text</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "      <th>publisher</th>\n",
       "      <th>retraction_binary</th>\n",
       "      <th>unpacked_keywords</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_text_lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27511111</td>\n",
       "      <td>10.1208/s12249-016-0596-x</td>\n",
       "      <td>eng</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>AAPS PharmSciTech</td>\n",
       "      <td>Study of the Transformations of Micro/Nano-cry...</td>\n",
       "      <td>1428-1437</td>\n",
       "      <td>‘Polymorphism’ generally referred as the abili...</td>\n",
       "      <td>This study elucidates the physical properties ...</td>\n",
       "      <td>['monoclinic', 'nano-sized crystals', 'orthorh...</td>\n",
       "      <td>Springer International Publishing</td>\n",
       "      <td>1</td>\n",
       "      <td>[monoclinic, nano-sized, crystals, orthorhombi...</td>\n",
       "      <td>Polymorphism generally referred as the ability...</td>\n",
       "      <td>Polymorphism generally referred a the ability ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31263767</td>\n",
       "      <td>10.1021/acscentsci.9b00224</td>\n",
       "      <td>eng</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>ACS central science</td>\n",
       "      <td>Targeted Protein Internalization and Degradati...</td>\n",
       "      <td>1079-1084</td>\n",
       "      <td>Traditional\\ndrug development efforts are focu...</td>\n",
       "      <td>Targeted</td>\n",
       "      <td>[]</td>\n",
       "      <td>American Chemical Society</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>Traditional drug development efforts are focus...</td>\n",
       "      <td>Traditional drug development effort are focuse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31458862</td>\n",
       "      <td>10.1021/acsomega.8b00488</td>\n",
       "      <td>eng</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>ACS omega</td>\n",
       "      <td>Regulating the Microstructure of Intumescent F...</td>\n",
       "      <td>6962-6970</td>\n",
       "      <td>Intumescent flame retardants\\nare now being us...</td>\n",
       "      <td>A compatibilizer</td>\n",
       "      <td>[]</td>\n",
       "      <td>American Chemical Society</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>Intumescent flame retardants are now being use...</td>\n",
       "      <td>Intumescent flame retardant are now being used...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31458855</td>\n",
       "      <td>10.1021/acsomega.8b00153</td>\n",
       "      <td>eng</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>ACS omega</td>\n",
       "      <td>Solid-to-Solid Crystallization of Organic Thin...</td>\n",
       "      <td>6874-6879</td>\n",
       "      <td>Crystal growth process is basic and essential ...</td>\n",
       "      <td>The solid-to-solid crystallization processes o...</td>\n",
       "      <td>[]</td>\n",
       "      <td>American Chemical Society</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>Crystal growth process is basic and essential ...</td>\n",
       "      <td>Crystal growth process is basic and essential ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21837091</td>\n",
       "      <td>10.1107/S1600536811022574</td>\n",
       "      <td>eng</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acta crystallographica. Section E, Structure r...</td>\n",
       "      <td>Oxonium picrate.</td>\n",
       "      <td>o1694</td>\n",
       "      <td>For general background to organic salts of pic...</td>\n",
       "      <td>The title compound, H3O+·C6H2N3O7</td>\n",
       "      <td>[]</td>\n",
       "      <td>International Union of Crystallography</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>For general background to organic salts of pic...</td>\n",
       "      <td>For general background to organic salt of picr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                         doi language    year  month   day  volume  \\\n",
       "0  27511111   10.1208/s12249-016-0596-x      eng  2016.0    8.0  10.0    18.0   \n",
       "1  31263767  10.1021/acscentsci.9b00224      eng  2019.0    5.0   9.0     5.0   \n",
       "2  31458862    10.1021/acsomega.8b00488      eng  2018.0    6.0  27.0     3.0   \n",
       "3  31458855    10.1021/acsomega.8b00153      eng  2018.0    6.0  25.0     3.0   \n",
       "4  21837091   10.1107/S1600536811022574      eng  2011.0    6.0  18.0    67.0   \n",
       "\n",
       "   issue                                            journal  \\\n",
       "0    5.0                                  AAPS PharmSciTech   \n",
       "1    6.0                                ACS central science   \n",
       "2    6.0                                          ACS omega   \n",
       "3    6.0                                          ACS omega   \n",
       "4    NaN  Acta crystallographica. Section E, Structure r...   \n",
       "\n",
       "                                               title       page  \\\n",
       "0  Study of the Transformations of Micro/Nano-cry...  1428-1437   \n",
       "1  Targeted Protein Internalization and Degradati...  1079-1084   \n",
       "2  Regulating the Microstructure of Intumescent F...  6962-6970   \n",
       "3  Solid-to-Solid Crystallization of Organic Thin...  6874-6879   \n",
       "4                                   Oxonium picrate.      o1694   \n",
       "\n",
       "                                                text  \\\n",
       "0  ‘Polymorphism’ generally referred as the abili...   \n",
       "1  Traditional\\ndrug development efforts are focu...   \n",
       "2  Intumescent flame retardants\\nare now being us...   \n",
       "3  Crystal growth process is basic and essential ...   \n",
       "4  For general background to organic salts of pic...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  This study elucidates the physical properties ...   \n",
       "1                                           Targeted   \n",
       "2                                   A compatibilizer   \n",
       "3  The solid-to-solid crystallization processes o...   \n",
       "4                  The title compound, H3O+·C6H2N3O7   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  ['monoclinic', 'nano-sized crystals', 'orthorh...   \n",
       "1                                                 []   \n",
       "2                                                 []   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "\n",
       "                                publisher  retraction_binary  \\\n",
       "0       Springer International Publishing                  1   \n",
       "1               American Chemical Society                  1   \n",
       "2               American Chemical Society                  1   \n",
       "3               American Chemical Society                  1   \n",
       "4  International Union of Crystallography                  1   \n",
       "\n",
       "                                   unpacked_keywords  \\\n",
       "0  [monoclinic, nano-sized, crystals, orthorhombi...   \n",
       "1                                                 []   \n",
       "2                                                 []   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  Polymorphism generally referred as the ability...   \n",
       "1  Traditional drug development efforts are focus...   \n",
       "2  Intumescent flame retardants are now being use...   \n",
       "3  Crystal growth process is basic and essential ...   \n",
       "4  For general background to organic salts of pic...   \n",
       "\n",
       "                                      clean_text_lem  \n",
       "0  Polymorphism generally referred a the ability ...  \n",
       "1  Traditional drug development effort are focuse...  \n",
       "2  Intumescent flame retardant are now being used...  \n",
       "3  Crystal growth process is basic and essential ...  \n",
       "4  For general background to organic salt of picr...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retract['clean_text'] = clean_text\n",
    "retract['clean_text_lem'] = clean_text_lem\n",
    "retract.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text of each article as it was originally received from the query needed to be cleaned in several ways. There were several symbols and numerical values that needed to be removed. To remove these characters, the RegexpTokenizer was used. Additionally, several \"words\" were in fact URLs or strings of characters that were not intelligible (as they may have been artifacts from mathematical equations in the article). Thus, any string that was above 45 characters was ignored. Once these cleaning steps had been taken, the text was saved in two different ways: one method with no further formatting, and one method where a lemmatizer was used. The now two different text bodies were then combined back together using spaces and added to new columns in the dataframe. Text that had no further formatting was saved as \"clean_text\" while lemmatized text was saved as \"clean_text_lem\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning No Retraction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_one = pd.read_csv('./text__no_retraction__0_5545.csv')\n",
    "text_two = pd.read_csv('./text__no_retraction__5545_6166.csv')\n",
    "text_three = pd.read_csv('./text__no_retraction__6166_end.csv')\n",
    "\n",
    "abstract_one = pd.read_csv('./abstract__no_retraction__0_5545.csv')\n",
    "abstract_two = pd.read_csv('./abstract__no_retraction__5545_6166.csv')\n",
    "abstract_three = pd.read_csv('./abstract__no_retraction__6166_end.csv')\n",
    "\n",
    "keywords_one = pd.read_csv('./keywords__no_retraction__0_5545.csv')\n",
    "keywords_two = pd.read_csv('./keywords__no_retraction__5545_6166.csv')\n",
    "keywords_three = pd.read_csv('./keywords__no_retraction__6166_end.csv')\n",
    "\n",
    "publisher_one = pd.read_csv('./publisher__no_retraction__0_5545.csv')\n",
    "publisher_two = pd.read_csv('./publisher__no_retraction__5545_6166.csv')\n",
    "publisher_three = pd.read_csv('./publisher__no_retraction__6166_end.csv')\n",
    "\n",
    "doi_one = pd.read_csv('./doi__no_retraction__0_5545.csv')\n",
    "doi_two = pd.read_csv('./doi__no_retraction__5545_6166.csv')\n",
    "doi_three = pd.read_csv('./doi__no_retraction__6166_end.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the \"Data Collection\" notebook, the script would have ideally run through the entire DOI column. However, when using this function, I often had 404 errors or the script would stop abruptly with no error message. For this reason, I decided to break the DOI column into three different sections to closely monitor the information and save the information I received more frequently. Because of this, three different CSV files were created, each labeled with what DOI index values were used when pulling the information. These CSV files are opened in the above script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5541</th>\n",
       "      <td>5541</td>\n",
       "      <td>Winter air pollution in Ulaanbaatar, Mongolia ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5542</th>\n",
       "      <td>5542</td>\n",
       "      <td>Chronic infection with hepatitis C virus (HCV)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5543</th>\n",
       "      <td>5543</td>\n",
       "      <td>California has one of the most highly engineer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5544</th>\n",
       "      <td>5544</td>\n",
       "      <td>Thiol-dependent cathepsins are found in all li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5545</th>\n",
       "      <td>5545</td>\n",
       "      <td>To estimate hepatitis C virus (HCV) viremic ra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                                  0\n",
       "5541        5541  Winter air pollution in Ulaanbaatar, Mongolia ...\n",
       "5542        5542  Chronic infection with hepatitis C virus (HCV)...\n",
       "5543        5543  California has one of the most highly engineer...\n",
       "5544        5544  Thiol-dependent cathepsins are found in all li...\n",
       "5545        5545  To estimate hepatitis C virus (HCV) viremic ra..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_one.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>To estimate hepatitis C virus (HCV) viremic ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Pollinators are crucial in almost all terrestr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The association of melanosis coli with the dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>In most plant species, repetitive DNA constitu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>In the last decades, there has been a great in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                  0\n",
       "0           0  To estimate hepatitis C virus (HCV) viremic ra...\n",
       "1           1  Pollinators are crucial in almost all terrestr...\n",
       "2           2  The association of melanosis coli with the dev...\n",
       "3           3  In most plant species, repetitive DNA constitu...\n",
       "4           4  In the last decades, there has been a great in..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_two.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>617</td>\n",
       "      <td>Bread wheat (Triticum aestivum L.) is one of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>618</td>\n",
       "      <td>Metabolic syndrome (MetS), defined as a comple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>619</td>\n",
       "      <td>Competitive learning techniques are being succ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>620</td>\n",
       "      <td>In South East Africa, about 100,000 years ago ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>621</td>\n",
       "      <td>In the text of González-Fernández [1] can be f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                                  0\n",
       "617         617  Bread wheat (Triticum aestivum L.) is one of t...\n",
       "618         618  Metabolic syndrome (MetS), defined as a comple...\n",
       "619         619  Competitive learning techniques are being succ...\n",
       "620         620  In South East Africa, about 100,000 years ago ...\n",
       "621         621  In the text of González-Fernández [1] can be f..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_two.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>In the text of González-Fernández [1] can be f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Infantile spasms (IS) are the defining seizure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The arylamine N-acetyltransferases are a famil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Several biomarkers have been proposed for ultr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Osteoporosis is a skeletal disease characteriz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                  0\n",
       "0           0  In the text of González-Fernández [1] can be f...\n",
       "1           1  Infantile spasms (IS) are the defining seizure...\n",
       "2           2  The arylamine N-acetyltransferases are a famil...\n",
       "3           3  Several biomarkers have been proposed for ultr...\n",
       "4           4  Osteoporosis is a skeletal disease characteriz..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_three.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I realized I had duplicated two query calls when completing the PMC no retraction querying for the no retraction data. Because of this, I had to drop the appropriate rows from the CSV files for each piece of information determined while querying. The rows were dropped in the script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_one = text_one.drop(5545, axis=0)\n",
    "text_two = text_two.drop(621, axis=0)\n",
    "abstract_one = abstract_one.drop(5545, axis=0)\n",
    "abstract_two = abstract_two.drop(621, axis=0)\n",
    "keywords_one = keywords_one.drop(5545, axis=0)\n",
    "keywords_two = keywords_two.drop(621, axis=0)\n",
    "publisher_one = publisher_one.drop(5545, axis=0)\n",
    "publisher_two = publisher_two.drop(621, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9613, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_total = pd.concat([text_one, text_two, text_three], axis =0)\n",
    "text_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9613, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doi_total = pd.concat([doi_one, doi_two, doi_three], axis =0)\n",
    "doi_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_total = pd.concat([abstract_one, abstract_two, abstract_three], axis =0)\n",
    "keywords_total = pd.concat([keywords_one, keywords_two, keywords_three], axis =0)\n",
    "publisher_total = pd.concat([publisher_one, publisher_two, publisher_three], axis =0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each CSV file for each piece of information from the PMC query was concatenated together by row. Once the dataframe was created for each piece of information, the dataframe was then saved as a CSV file in the script below for proofing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_total.to_csv('./plos_only_no_retraction_text.csv')\n",
    "doi_total.to_csv('./plos_only_no_retraction_doi.csv')\n",
    "abstract_total.to_csv('./plos_only_no_retraction_abstract.csv')\n",
    "keywords_total.to_csv('./plos_only_no_retraction_keywords.csv')\n",
    "publisher_total.to_csv('./plos_only_no_retraction_publisher.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_csv('./plos_only_no_retraction_text.csv')\n",
    "abstract = pd.read_csv('./plos_only_no_retraction_abstract.csv')\n",
    "keywords = pd.read_csv('./plos_only_no_retraction_keywords.csv')\n",
    "publisher = pd.read_csv('./plos_only_no_retraction_publisher.csv')\n",
    "doi = pd.read_csv('./plos_only_no_retraction_doi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9613\n",
      "9613\n",
      "9613\n",
      "9613\n",
      "9613\n"
     ]
    }
   ],
   "source": [
    "#pulling all text data for no retractions\n",
    "ls_text = []\n",
    "for i in text['0']:\n",
    "    ls_text.append(i)\n",
    "print(len(ls_text))\n",
    "\n",
    "#pulling all abstract data for no retractions\n",
    "ls_abstract = []\n",
    "for i in abstract['0']:\n",
    "    ls_abstract.append(i)\n",
    "print(len(ls_abstract))\n",
    "\n",
    "#pulling all keywords list data for no retractions\n",
    "ls_keywords = []\n",
    "for i in keywords['0']:\n",
    "    ls_keywords.append(i)\n",
    "print(len(ls_keywords))\n",
    "\n",
    "#pulling all publisher data for no retractions\n",
    "ls_publisher = []\n",
    "for i in publisher['0']:\n",
    "    ls_publisher.append(i)\n",
    "print(len(ls_publisher))\n",
    "\n",
    "#pulling all DOI data for no retractions\n",
    "ls_doi = []\n",
    "for i in doi['1']:\n",
    "    ls_doi.append(i)\n",
    "print(len(ls_doi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To place all of the information from the PMC no retraction query, the information from the newly formed CSV files was pulled and placed into a list for each information piece. The PubMed no retraction CSV file was then opened. All of the lists were concatenated with the PubMed no retraction dataframe to combine all of the no retraction inforamtion into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_retract = pd.read_csv('./pubmed_data_second_no_retraction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_retract = pd.concat([no_retract, pd.Series(ls_text), pd.Series(ls_abstract), pd.Series(ls_keywords), \n",
    "                        pd.Series(ls_publisher), pd.Series(ls_doi)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_retract = no_retract.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_retract = no_retract.rename(columns={'0':'id', '1':'doi', '2':'language', '3':'year', '4':'month', '5':'day', \n",
    "                      '6':'volume', '7':'issue', '8':'journal', '9':'title', '10':'page', \n",
    "                      0:'text', 1:'abstract', 2:'keywords', 3:'publisher', 4:'doi_check'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unnecessary columns were dropped. For the columns that remain, the columns were renamed to the appropriate information based on the order in which the information was pulled during querying. The complete dataframe can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>doi</th>\n",
       "      <th>language</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>volume</th>\n",
       "      <th>issue</th>\n",
       "      <th>journal</th>\n",
       "      <th>title</th>\n",
       "      <th>page</th>\n",
       "      <th>text</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "      <th>publisher</th>\n",
       "      <th>doi_check</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25569838</td>\n",
       "      <td>10.1371/journal.pone.0115528</td>\n",
       "      <td>eng</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>PloS one</td>\n",
       "      <td>High incidence is not high exposure: what prop...</td>\n",
       "      <td>e0115528</td>\n",
       "      <td>Randomized clinical trials of HIV prevention i...</td>\n",
       "      <td>Objective</td>\n",
       "      <td>[]</td>\n",
       "      <td>Public Library of Science</td>\n",
       "      <td>10.1371/journal.pone.0115528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25569796</td>\n",
       "      <td>10.1371/journal.pone.0115194</td>\n",
       "      <td>eng</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>PloS one</td>\n",
       "      <td>Neurological abnormalities in full-term asphyx...</td>\n",
       "      <td>e0115194</td>\n",
       "      <td>Perinatal asphyxia (PA) is a leading cause of ...</td>\n",
       "      <td>Background</td>\n",
       "      <td>[]</td>\n",
       "      <td>Public Library of Science</td>\n",
       "      <td>10.1371/journal.pone.0115194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25569682</td>\n",
       "      <td>10.1371/journal.pone.0117040</td>\n",
       "      <td>eng</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>PloS one</td>\n",
       "      <td>Production of siderophores increases resistanc...</td>\n",
       "      <td>e0117040</td>\n",
       "      <td>Fusaric acid (FA, 5-butylpyridine-2-carboxylic...</td>\n",
       "      <td>Fusaric acid is produced by pathogenic fungi o...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Public Library of Science</td>\n",
       "      <td>10.1371/journal.pone.0117040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25569558</td>\n",
       "      <td>10.1371/journal.pone.0116930</td>\n",
       "      <td>eng</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>PloS one</td>\n",
       "      <td>Application of clinico-radiologic-pathologic d...</td>\n",
       "      <td>e0116930</td>\n",
       "      <td>Diffuse parenchymal lung diseases in children ...</td>\n",
       "      <td>Diffuse parenchymal lung diseases in children ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Public Library of Science</td>\n",
       "      <td>10.1371/journal.pone.0116930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25569428</td>\n",
       "      <td>10.1371/journal.pone.0116566</td>\n",
       "      <td>eng</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>PloS one</td>\n",
       "      <td>Cinnamon ameliorates experimental allergic enc...</td>\n",
       "      <td>e0116566</td>\n",
       "      <td>Regulatory T cells (Tregs) are regarded as the...</td>\n",
       "      <td>Upregulation and/or maintenance of regulatory ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Public Library of Science</td>\n",
       "      <td>10.1371/journal.pone.0116566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                           doi language  year  month  day  volume  \\\n",
       "0  25569838  10.1371/journal.pone.0115528      eng  2015      1    8      10   \n",
       "1  25569796  10.1371/journal.pone.0115194      eng  2015      1    8      10   \n",
       "2  25569682  10.1371/journal.pone.0117040      eng  2015      1    8      10   \n",
       "3  25569558  10.1371/journal.pone.0116930      eng  2015      1    8      10   \n",
       "4  25569428  10.1371/journal.pone.0116566      eng  2015      1    8      10   \n",
       "\n",
       "   issue   journal                                              title  \\\n",
       "0      1  PloS one  High incidence is not high exposure: what prop...   \n",
       "1      1  PloS one  Neurological abnormalities in full-term asphyx...   \n",
       "2      1  PloS one  Production of siderophores increases resistanc...   \n",
       "3      1  PloS one  Application of clinico-radiologic-pathologic d...   \n",
       "4      1  PloS one  Cinnamon ameliorates experimental allergic enc...   \n",
       "\n",
       "       page                                               text  \\\n",
       "0  e0115528  Randomized clinical trials of HIV prevention i...   \n",
       "1  e0115194  Perinatal asphyxia (PA) is a leading cause of ...   \n",
       "2  e0117040  Fusaric acid (FA, 5-butylpyridine-2-carboxylic...   \n",
       "3  e0116930  Diffuse parenchymal lung diseases in children ...   \n",
       "4  e0116566  Regulatory T cells (Tregs) are regarded as the...   \n",
       "\n",
       "                                            abstract keywords  \\\n",
       "0                                          Objective       []   \n",
       "1                                         Background       []   \n",
       "2  Fusaric acid is produced by pathogenic fungi o...       []   \n",
       "3  Diffuse parenchymal lung diseases in children ...       []   \n",
       "4  Upregulation and/or maintenance of regulatory ...       []   \n",
       "\n",
       "                   publisher                     doi_check  \n",
       "0  Public Library of Science  10.1371/journal.pone.0115528  \n",
       "1  Public Library of Science  10.1371/journal.pone.0115194  \n",
       "2  Public Library of Science  10.1371/journal.pone.0117040  \n",
       "3  Public Library of Science  10.1371/journal.pone.0116930  \n",
       "4  Public Library of Science  10.1371/journal.pone.0116566  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_retract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5540    10.1371/journal.pone.0186821\n",
       "5541    10.1371/journal.pone.0186834\n",
       "5542    10.1371/journal.pone.0186898\n",
       "5543    10.1371/journal.pone.0187181\n",
       "5544    10.1371/journal.pone.0186869\n",
       "5545    10.1371/journal.pone.0187177\n",
       "5546    10.1371/journal.pone.0187079\n",
       "5547    10.1371/journal.pone.0186668\n",
       "5548    10.1371/journal.pone.0187131\n",
       "5549    10.1371/journal.pone.0186957\n",
       "Name: doi, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_retract['doi'][5540:5550]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5540    10.1371/journal.pone.0186821\n",
       "5541    10.1371/journal.pone.0186834\n",
       "5542    10.1371/journal.pone.0186898\n",
       "5543    10.1371/journal.pone.0187181\n",
       "5544    10.1371/journal.pone.0186869\n",
       "5545    10.1371/journal.pone.0187177\n",
       "5546    10.1371/journal.pone.0187079\n",
       "5547    10.1371/journal.pone.0186668\n",
       "5548    10.1371/journal.pone.0187131\n",
       "5549    10.1371/journal.pone.0186957\n",
       "Name: doi_check, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_retract['doi_check'][5540:5550]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_retract.to_csv('./plos_only_no_retraction_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_retract = pd.read_csv('./plos_only_no_retraction_data.csv', index_col=False)\n",
    "no_retract = no_retract.drop(columns=['Unnamed: 0', 'doi_check'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DOI values were compared between the two different DOI columns to ensure that the concatenating between the different dataframes was completed accurately. Once it was determined that there were no errors in concatenating, the dataframe was saved as a new CSV file for proofing. When reading the file back into the notebook to continue cleaning, the appropriate columns were dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494\n",
      "399\n"
     ]
    }
   ],
   "source": [
    "print(no_retract['abstract'].isnull().sum())\n",
    "print(no_retract['text'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above script shows the number of null values in the \"abstract\" and \"text\" column. In the case that there is simply too much text in the full article to be used for NLP modeling, it may be possible to use the information in the abstract of each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_retract = no_retract.dropna(axis=0, subset=['text'])\n",
    "print(no_retract.shape)\n",
    "no_retract.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any articles that were missing text were dropped from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10.1371/journal.pone.0221236', '10.1371/journal.pone.0208797',\n",
       "       '10.1371/journal.pone.0145158', '10.1371/journal.pone.0184077',\n",
       "       '10.1371/journal.pone.0169155', '10.1371/journal.pone.0119129',\n",
       "       '10.1371/journal.pone.0175673', '10.1371/journal.pone.0226358',\n",
       "       '10.1371/journal.pone.0186309', '10.1371/journal.pone.0144882',\n",
       "       '10.1371/journal.pone.0217685', '10.1371/journal.pone.0168498',\n",
       "       '10.1371/journal.pone.0203429', '10.1371/journal.pone.0147806',\n",
       "       '10.1371/journal.pone.0210432', '10.1371/journal.pone.0201007',\n",
       "       '10.1371/journal.pone.0155713', '10.1371/journal.pone.0178620',\n",
       "       '10.1371/journal.pone.0135598', '10.1371/journal.pone.0178231',\n",
       "       '10.1371/journal.pone.0156508', '10.1371/journal.pone.0118722',\n",
       "       '10.1371/journal.pone.0179354', '10.1371/journal.pone.0221109',\n",
       "       '10.1371/journal.pone.0171148', '10.1371/journal.pone.0152195',\n",
       "       '10.1371/journal.pone.0131134', '10.1371/journal.pone.0216493',\n",
       "       '10.1371/journal.pone.0216062', '10.1371/journal.pone.0221297',\n",
       "       '10.1371/journal.pone.0136188', '10.1371/journal.pone.0214864',\n",
       "       '10.1371/journal.pone.0213110', '10.1371/journal.pone.0194065',\n",
       "       '10.1371/journal.pone.0224176', '10.1371/journal.pone.0168330',\n",
       "       '10.1371/journal.pone.0204061', '10.1371/journal.pone.0134110',\n",
       "       '10.1371/journal.pone.0176478', '10.1371/journal.pone.0182935',\n",
       "       '10.1371/journal.pone.0212236', '10.1371/journal.pone.0200779',\n",
       "       '10.1371/journal.pone.0174632', '10.1371/journal.pone.0195339',\n",
       "       '10.1371/journal.pone.0184493', '10.1371/journal.pone.0224195',\n",
       "       '10.1371/journal.pone.0152406', '10.1371/journal.pone.0156847',\n",
       "       '10.1371/journal.pone.0118853', '10.1371/journal.pone.0221362'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(no_retract['doi'], 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50 DOIs of articles that were not retracted from PLOS ONE were pulled. These DOIs were used to manually check that the articles pulled during the query were in fact not retracted. While all of the articles proved to be not retracted, 5 of the articles had actually been corrected. The reasons for correction were as followed: author byline changes and fixing a citation, supporting figure appears incorrectly, authors spelled incorrectly, errors in figures, and affiliation listing error. Corrections are not the same as retractions, but may be something to look into in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_retract['retraction_binary'] = 0\n",
    "print(no_retract['retraction_binary'].value_counts())\n",
    "no_retract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_retract.to_csv('./no_null_text_plos_only_no_retraction_data.csv')\n",
    "no_retract = pd.read_csv('./no_null_text_plos_only_no_retraction_data.csv')\n",
    "no_retract = no_retract.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new column named \"retraction_binary\" was created that was filled with the integer 0 to indicate that all of the articles in this dataframe were articles that had not been retracted. Using .info method below, it is evident that the only remaining articles had full text. Once the data was cleaned, it was saved into a new CSV file for proofing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9214 entries, 0 to 9213\n",
      "Data columns (total 16 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   id                 9214 non-null   int64 \n",
      " 1   doi                9214 non-null   object\n",
      " 2   language           9214 non-null   object\n",
      " 3   year               9214 non-null   int64 \n",
      " 4   month              9214 non-null   int64 \n",
      " 5   day                9214 non-null   int64 \n",
      " 6   volume             9214 non-null   int64 \n",
      " 7   issue              9214 non-null   int64 \n",
      " 8   journal            9214 non-null   object\n",
      " 9   title              9214 non-null   object\n",
      " 10  page               9214 non-null   object\n",
      " 11  text               9214 non-null   object\n",
      " 12  abstract           9119 non-null   object\n",
      " 13  keywords           9214 non-null   object\n",
      " 14  publisher          9214 non-null   object\n",
      " 15  retraction_binary  9214 non-null   int64 \n",
      "dtypes: int64(7), object(9)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "no_retract.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]                                                                            9213\n",
       "['Socioeconomic inequality', 'Antenatal care', 'Decomposition', 'Nigeria']       1\n",
       "Name: keywords, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_retract['keywords'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I went to unpack the keywords list as I had done for the retraction data. However, after completing .value_counts method for the column, I realized that only one data point out of 9214 articles actually had a keywords list. I manually checked using the same 50 random DOI values to determine that none of the articles randomly chosen actually had a keywords list. Because of this, I did not bother to unpack the keywords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "clean_text = []\n",
    "clean_text_lem = []\n",
    "\n",
    "for i in range(0, len(no_retract['text'])):\n",
    "    ls_words = []\n",
    "    ls_lem = []\n",
    "    for j in tokenizer.tokenize(no_retract['text'][i]):\n",
    "        try:\n",
    "            int(j)\n",
    "        except:\n",
    "            if len(j) < 45:\n",
    "                ls_words.append(j)\n",
    "                ls_lem.append(lemmatizer.lemmatize(j))\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "    clean_text.append(' '.join(ls_words))\n",
    "    clean_text_lem.append(' '.join(ls_lem))\n",
    "    \n",
    "    print(i)\n",
    "\n",
    "print(len(no_retract['text'][1]))\n",
    "print(len(clean_text[1]))\n",
    "print(len(clean_text_lem[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_retract['clean_text'] = clean_text\n",
    "no_retract['clean_text_lem'] = clean_text_lem\n",
    "no_retract.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text of each article as it was originally received from the query needed to be cleaned in several ways. There were several symbols and numerical values that needed to be removed. To remove these characters, the RegexpTokenizer was used. Additionally, several \"words\" were in fact URLs or strings of characters that were not intelligible (as they may have been artifacts from mathematical equations in the article). Thus, any string that was above 45 characters was ignored. Once these cleaning steps had been taken, the text was saved in two different ways: one method with no further formatting, and one method where a lemmatizer was used. The now two different text bodies were then combined back together using spaces and added to new columns in the dataframe. Text that had no further formatting was saved as \"clean_text\" while lemmatized text was saved as \"clean_text_lem\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "            ...\n",
       "            157, 158, 159, 160, 161, 162, 163, 164, 165, 166],\n",
       "           dtype='int64', length=124)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_retract.loc[no_retract['year']==2014].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2019    1891\n",
       "2016    1833\n",
       "2015    1792\n",
       "2018    1788\n",
       "2017    1786\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_retract = no_retract.drop(range(43, 167))\n",
    "no_retract['year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the querying process, the dates randomly chosen were used as the end point of the query. Thus, 167 articles were pulled where each date is the last possible date the article could have been published on. Because of this, there were 124 articles that were published in 2014. For consistency, these articles were dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Combined Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pd.concat([retract, no_retract], axis=0)\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retraction and no retraction dataframes were concatenated together to create a dataframe that has all of the inforamtion from all of the queries completed in the \"Data Collection\" notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = total.drop(columns=['id', 'language', 'publisher'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26          7\n",
       "4           5\n",
       "123         4\n",
       "85          4\n",
       "34          4\n",
       "           ..\n",
       "e0181077    1\n",
       "e0189277    1\n",
       "3852        1\n",
       "e0186435    1\n",
       "e0196047    1\n",
       "Name: page, Length: 10451, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total['page'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = total.drop(columns=['page'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "total['year'] = total['year'].astype(str)\n",
    "total['month'] = total['month'].astype(str)\n",
    "total['day'] = total['day'].astype(str)\n",
    "total['volume'] = total['volume'].astype(str)\n",
    "total['issue'] = total['issue'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unnecessary columns were dropped. The \"year,\" \"month,\" \"day,\" \"volume,\" and \"issue\" columns were retyped as strings so that they could be dummied in later notebooks. The .info method shows that there are no null values in the columns necessary for NLP modeling. Once cleaned, the dataframe was saved into a new CSV file for proofing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10643 entries, 0 to 9213\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   doi                10643 non-null  object\n",
      " 1   year               10643 non-null  object\n",
      " 2   month              10643 non-null  object\n",
      " 3   day                10643 non-null  object\n",
      " 4   volume             10643 non-null  object\n",
      " 5   issue              10643 non-null  object\n",
      " 6   journal            10643 non-null  object\n",
      " 7   title              10643 non-null  object\n",
      " 8   text               10643 non-null  object\n",
      " 9   abstract           10528 non-null  object\n",
      " 10  keywords           10643 non-null  object\n",
      " 11  retraction_binary  10643 non-null  int64 \n",
      " 12  unpacked_keywords  1553 non-null   object\n",
      " 13  clean_text         10643 non-null  object\n",
      " 14  clean_text_lem     10643 non-null  object\n",
      "dtypes: int64(1), object(14)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "total.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.to_csv('./total_plos_only_data.csv')\n",
    "total = pd.read_csv('./total_plos_only_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.1074/jbc.M111.329078           4\n",
       "10.1371/journal.pone.0194078      2\n",
       "10.1038/cr.2011.194               2\n",
       "10.1074/jbc.M111.275073           2\n",
       "10.1371/journal.pone.0164378      2\n",
       "10.1038/cdd.2010.114              2\n",
       "10.1371/journal.pone.0212021      2\n",
       "10.1074/jbc.M110.175802           2\n",
       "10.3390/nano4020203               2\n",
       "10.1074/jbc.M808084200            2\n",
       "10.1371/journal.pone.0216079      2\n",
       "10.1371/journal.pone.0155697      2\n",
       "10.1523/JNEUROSCI.0372-13.2013    2\n",
       "10.1371/journal.pone.0146671      2\n",
       "10.1074/jbc.M112.387738           2\n",
       "10.1200/JCO.2017.74.7824          2\n",
       "10.1155/2012/236409               2\n",
       "10.1074/jbc.M709854200            2\n",
       "10.1371/journal.pone.0140044      2\n",
       "10.1371/journal.pone.0125542      2\n",
       "10.3389/fnins.2018.00529          2\n",
       "10.1371/journal.pone.0183066      1\n",
       "10.1371/journal.pone.0225345      1\n",
       "10.1371/journal.pone.0193981      1\n",
       "10.1371/journal.pone.0166478      1\n",
       "Name: doi, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total['doi'].value_counts().head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = total.drop_duplicates(subset='doi', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.1371/journal.pone.0156737    1\n",
       "10.1371/journal.pone.0164836    1\n",
       "10.1371/journal.pone.0168679    1\n",
       "10.1371/journal.pone.0210251    1\n",
       "10.1371/journal.pone.0168758    1\n",
       "Name: doi, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total['doi'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the querying process, some DOI values were repeated even though duplicates should have been dropped before pulling from the PMC database. These DOI values are unique strings that identify a specific article. Because of this, if a DOI is repeated, then the article text is repeated within the dataframe as well. Thus, these duplicates were dropped and saved to a new CSV file for proofing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.to_csv('./total_plos_only_data_no_duplicates.csv')\n",
    "total = pd.read_csv('./total_plos_only_data_no_duplicates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "950    For the chemotherapeutic activity of pyrimidin...\n",
       "951    Urinary tract infection UTI is a bacterial inf...\n",
       "952    The Bible descrbies the case of a woman with h...\n",
       "953                                                  NaN\n",
       "954    Human betaherpesviruses 6A and 6B HHV 6A and H...\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total['clean_text'][950:955]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "950    For the chemotherapeutic activity of pyrimidin...\n",
       "951    Urinary tract infection UTI is a bacterial inf...\n",
       "952    The Bible descrbies the case of a woman with h...\n",
       "954    Human betaherpesviruses 6A and 6B HHV 6A and H...\n",
       "955    A cesarean section is the most frequently perf...\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = total.drop(953)\n",
    "total['clean_text'][950:955]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the tokenizing process, one text must have been removed completely (as the text may not have had any spaces). This row was dropped as it serves no purpose to modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1537     1\n",
       "1538     0\n",
       "1539     0\n",
       "1540     0\n",
       "1541     0\n",
       "        ..\n",
       "10615    0\n",
       "10616    0\n",
       "10617    0\n",
       "10618    0\n",
       "10619    0\n",
       "Name: retraction_binary, Length: 9083, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total['retraction_binary'][1536:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.to_csv('./total_data_plos_only_cleaned.csv')\n",
    "total[1537:].to_csv('./no_retraction_data_plos_only_cleaned.csv')\n",
    "total[:1537].to_csv('./retraction_data_plos_only_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the way the dataframes were concatenated, the dataframe from index 0-1537 are only retracted articles while the index 1537-10643 are only non-retracted articles. Thus, the final dataframe was saved in three different ways: the complete dataframe, only retracted articles, and only non-retracted articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
