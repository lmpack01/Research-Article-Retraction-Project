{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import textstat\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pd.read_csv('./total_data_plos_only_cleaned.csv')\n",
    "total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = total.drop(columns=['Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_retract = pd.read_csv('./no_retraction_data_plos_only_cleaned.csv')\n",
    "no_retract = no_retract.drop(columns=['Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "retract = pd.read_csv('./retraction_data_plos_only_cleaned.csv')\n",
    "retract = retract.drop(columns=['Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last CSV files created in the \"Data Cleaning\" notebook were read and unnecessary columns were dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keywords lists in the retraction dataframe were unpacked and lemmatized to determine the most common keywords in retracted articles. This information will be explored in the \"EDA\" notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This script was influenced by https://stackoverflow.com/questions/40950791/remove-quotes-from-string-in-python\n",
    "keywords_list = []\n",
    "count = 0\n",
    "for i in retract['keywords']:\n",
    "    if i == []: #if no keywords, move to next article\n",
    "        pass\n",
    "    else:\n",
    "        for j in i.split(): #splitting each keyword string\n",
    "            \n",
    "            #removing all symbols from each split, lowercasing all remaining words, and adding to a list\n",
    "            keywords_list.append(j.replace(\"'\",'').replace('[','').replace(',','').replace(']','').replace('(','').replace(')','').replace('\\\\n', '').replace('\\\\n','').lower())\n",
    "            \n",
    "#output is a list that contains each occurrence of each keyword discussed in retracted articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            663\n",
       "cell        186\n",
       "antibody    116\n",
       "cancer      112\n",
       "response     89\n",
       "disease      84\n",
       "health       79\n",
       "factor       71\n",
       "theory       67\n",
       "gene         66\n",
       "heat         61\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "ls_keywords = []\n",
    "for i in keywords_list:\n",
    "    ls_keywords.append(lemmatizer.lemmatize(i)) #lemmatizing list of keywords produced above to increase topic diversity\n",
    "pd.Series(ls_keywords).value_counts().head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keywords Binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyword binary columns were created in the total dataframe and the retraction dataframe. If there is no keyword list in the article, then the value is set to 0. If there is a keyword list in the article, then the value is set to 1. This information will be explored in the \"EDA\" notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10619\n"
     ]
    }
   ],
   "source": [
    "keywords_binary = []\n",
    "for i in total['keywords']:\n",
    "    if len(i) != 2:\n",
    "        keywords_binary.append(1)\n",
    "    else:\n",
    "        keywords_binary.append(0)\n",
    "print(len(keywords_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total['keywords_binary'] = keywords_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537\n"
     ]
    }
   ],
   "source": [
    "keywords_binary = []\n",
    "for i in retract['keywords']:\n",
    "    if len(i) != 2:\n",
    "        keywords_binary.append(1)\n",
    "    else:\n",
    "        keywords_binary.append(0)\n",
    "print(len(keywords_binary))\n",
    "\n",
    "retract['keywords_binary'] = keywords_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word count column was created using the in \"clean_text\" column the total dataframe to represent the number of words in each article. This information will be explored in the \"EDA\" notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_words = []\n",
    "for i in range(0, len(total['clean_text'])):\n",
    "    list_words.append(len(total['clean_text'][i].split()))\n",
    "total['num_words'] = list_words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A character length column was created using the in \"clean_text\" column the total dataframe to represent the number of characters in each article. This information will be explored in the \"EDA\" notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_words = []\n",
    "for i in range(0, len(total['clean_text'])):\n",
    "    list_words.append(len(total['clean_text'][i]))\n",
    "total['character_length'] = list_words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animal Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I thought that certain topics (such as animal-based research) would have more regulatory bodies, thus would be less likely to be retracted because more people are ensuring the methods and results of the project. Additionally, working with animals or animal-based products can prove to be much more variated than mathematical models. Because of this, I wanted to be able to explore words related to animals or animal studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_terms = ['IACUC', 'mouse', 'mice', 'rats', 'rat', 'hamster', 'hamsters', 'pigs', 'rabbits', 'rabbit', \n",
    "                'cat', 'cats', 'dog', 'dogs', 'ungulate', 'ungulates', 'pig', 'horse', 'donkey', 'goat',\n",
    "               'bovine', 'porcine', 'murine', 'chicken', 'sheep', 'cow', 'cows', 'horses', 'goats']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To search for words related to animals, the target words must be decided upon prior to searching through the article. [Source 1](http://vetmed.tamu.edu/media/2005639/vadnais%20protein%20therapeutics%202017.pdf) and [source 2](https://www.ncbi.nlm.nih.gov/books/NBK218261/) were used as resources for determining appropriate animal study words. These words were put into the list above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animal_binary(dataframe):\n",
    "    list_articles = []\n",
    "    iacuc = []\n",
    "    \n",
    "    #passing through each article\n",
    "    for i in range(0, len(dataframe['clean_text'])):\n",
    "        count = 0\n",
    "        \n",
    "        #passing through each word in each article\n",
    "        for j in dataframe['clean_text'][i].split():\n",
    "            \n",
    "            #comparing the word in the article to the list of animal terms\n",
    "            for k in animal_terms:\n",
    "                if j == k:\n",
    "                    \n",
    "                    #prevents the article from being repeatedly added to the lists\n",
    "                    if i not in list_articles:\n",
    "                        list_articles.append(i)\n",
    "                        iacuc.append(1)\n",
    "                        count = 1\n",
    "                else:\n",
    "                    pass\n",
    "        \n",
    "        #used for if the article does not contain any animal terms\n",
    "        if count == 0:\n",
    "            iacuc.append(0)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    #shows the number of articles that contained animal terms\n",
    "    print(len(list_articles))\n",
    "    \n",
    "    #shows if there will be a reshape error when adding to the dataframe\n",
    "    print(len(iacuc))\n",
    "\n",
    "    dataframe['animal_binary'] = iacuc\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4289\n",
      "10619\n",
      "791\n",
      "1537\n",
      "3498\n",
      "9082\n"
     ]
    }
   ],
   "source": [
    "animal_binary(total)\n",
    "animal_binary(retract)\n",
    "animal_binary(no_retract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a function in the script above to identify if an article contained one of these animal terms. A column was created that contained binary values, where 0 indicated the article did not contain any of the animal terms while a 1 indicated the article contained at least one instance of one of the animal terms. This information will be explored in the \"EDA\" notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_animal_words(dataframe):\n",
    "    list_articles = []\n",
    "    list_words = []\n",
    "    \n",
    "    #passing through each article\n",
    "    for i in range(0, len(dataframe['clean_text'])):\n",
    "        count = 0\n",
    "        iacuc = []\n",
    "        \n",
    "        #passing through each word in each article\n",
    "        for j in dataframe['clean_text'][i].split():\n",
    "            \n",
    "            #comparing the word in the article to the list of animal terms\n",
    "            for k in animal_terms:\n",
    "                if j == k:\n",
    "                    \n",
    "                    #prevents the article from being repeatedly added to the lists\n",
    "                    if i not in list_articles:\n",
    "                        list_articles.append(i)\n",
    "                    \n",
    "                    #prevents an animal term from being repeatedly added to the list for each article\n",
    "                    if j not in iacuc:\n",
    "                        iacuc.append(k)\n",
    "                    count = 1\n",
    "                else:\n",
    "                    pass\n",
    "        \n",
    "        #used for if the article does not contain any animal terms\n",
    "        if count == 0:\n",
    "            list_words.append([])\n",
    "        else:\n",
    "            list_words.append(iacuc)\n",
    "    \n",
    "    #shows the number of articles that contained animal terms\n",
    "    print(len(list_articles))\n",
    "    \n",
    "    #shows if there will be a reshape error when adding to the dataframe\n",
    "    print(len(list_words))\n",
    "    dataframe['animal_words'] = list_words\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4289\n",
      "10619\n",
      "791\n",
      "1537\n",
      "3498\n",
      "9082\n"
     ]
    }
   ],
   "source": [
    "list_of_animal_words(total)\n",
    "list_of_animal_words(retract)\n",
    "list_of_animal_words(no_retract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also wanted to look at the distribution of animal terms in each article, as immunostaining will require several different animal-based products. Thus, I created a column of lists, where each list shows which animal term appeared in the article. This information will be explored in the \"EDA\" notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iacuc = ['IACUC']\n",
    "mouse = ['mouse', 'mice']\n",
    "rat = ['rat', 'rats']\n",
    "murine = ['murine']\n",
    "hamster = ['hamster', 'hamsters']\n",
    "rabbit = ['rabbit', 'rabbits']\n",
    "cat = ['cat', 'cats']\n",
    "pig = ['pig', 'pigs', 'porcine']\n",
    "dog = ['dog', 'dogs']\n",
    "ungulate = ['ungulate', 'ungulates']\n",
    "horse = ['horse', 'horses']\n",
    "donkey = ['donkey']\n",
    "goat = ['goat', 'goats']\n",
    "cow = ['cow', 'cows', 'bovine']\n",
    "chicken = ['chicken']\n",
    "sheep = ['sheep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new animal dummy column\n",
    "def animal_dummy(word_list, column_name, dataframe):\n",
    "    column_list = []\n",
    "    \n",
    "    for i in dataframe['animal_words']: #each animal word list for each article\n",
    "        count = 0\n",
    "        for j in i: #each animal word\n",
    "            for k in word_list: #each category of animal\n",
    "                if j == k:\n",
    "                    count = 1 #set count = 1 if one of the words in the animal category appears in the text\n",
    "                    \n",
    "        #the number of times one of the words that indicates a certain category of animal\n",
    "        column_list.append(count) \n",
    "    \n",
    "    dataframe[column_name] = column_list\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I needed to unpack the lists of animal terms, but wanted to combine words that were related into similar categories. I created the above function and then ran it for each category for each dataframe in the script below. In a way, this dummies the \"animal_words\" column. This information will be explored in the \"EDA\" notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_dummy(iacuc, 'iacuc', retract)\n",
    "animal_dummy(mouse, 'mouse', retract)\n",
    "animal_dummy(rat, 'rat', retract)\n",
    "animal_dummy(murine, 'murine', retract)\n",
    "animal_dummy(hamster, 'hamster', retract)\n",
    "animal_dummy(rabbit, 'rabbit', retract)\n",
    "animal_dummy(cat, 'cat', retract)\n",
    "animal_dummy(pig, 'pig', retract)\n",
    "animal_dummy(dog, 'dog', retract)\n",
    "animal_dummy(ungulate, 'ungulate', retract)\n",
    "animal_dummy(horse, 'horse', retract)\n",
    "animal_dummy(donkey, 'donkey', retract)\n",
    "animal_dummy(goat, 'goat', retract)\n",
    "animal_dummy(cow, 'cow', retract)\n",
    "animal_dummy(chicken, 'chicken', retract)\n",
    "animal_dummy(sheep, 'sheep', retract)\n",
    "\n",
    "animal_dummy(iacuc, 'iacuc', no_retract)\n",
    "animal_dummy(mouse, 'mouse', no_retract)\n",
    "animal_dummy(rat, 'rat', no_retract)\n",
    "animal_dummy(murine, 'murine', no_retract)\n",
    "animal_dummy(hamster, 'hamster', no_retract)\n",
    "animal_dummy(rabbit, 'rabbit', no_retract)\n",
    "animal_dummy(cat, 'cat', no_retract)\n",
    "animal_dummy(pig, 'pig', no_retract)\n",
    "animal_dummy(dog, 'dog', no_retract)\n",
    "animal_dummy(ungulate, 'ungulate', no_retract)\n",
    "animal_dummy(horse, 'horse', no_retract)\n",
    "animal_dummy(donkey, 'donkey', no_retract)\n",
    "animal_dummy(goat, 'goat', no_retract)\n",
    "animal_dummy(cow, 'cow', no_retract)\n",
    "animal_dummy(chicken, 'chicken', no_retract)\n",
    "animal_dummy(sheep, 'sheep', no_retract)\n",
    "\n",
    "animal_dummy(iacuc, 'iacuc', total)\n",
    "animal_dummy(mouse, 'mouse', total)\n",
    "animal_dummy(rat, 'rat', total)\n",
    "animal_dummy(murine, 'murine', total)\n",
    "animal_dummy(hamster, 'hamster', total)\n",
    "animal_dummy(rabbit, 'rabbit', total)\n",
    "animal_dummy(cat, 'cat', total)\n",
    "animal_dummy(pig, 'pig', total)\n",
    "animal_dummy(dog, 'dog', total)\n",
    "animal_dummy(ungulate, 'ungulate', total)\n",
    "animal_dummy(horse, 'horse', total)\n",
    "animal_dummy(donkey, 'donkey', total)\n",
    "animal_dummy(goat, 'goat', total)\n",
    "animal_dummy(cow, 'cow', total)\n",
    "animal_dummy(chicken, 'chicken', total)\n",
    "animal_dummy(sheep, 'sheep', total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below .value_counts methods show that there was a increase of retracted articles containing an animal term of approximately 13% compared to non-retracted articles. Thus, articles that were retracted were more likely to contain an animal term. It is possible that the variance in working with animal models correlates to an article being retracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.514639\n",
       "0    0.485361\n",
       "Name: animal_binary, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retracted articles\n",
    "total['animal_binary'][:1537].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.614843\n",
       "1    0.385157\n",
       "Name: animal_binary, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#non-retracted articles\n",
    "total['animal_binary'][1537:].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the animal studies feature engineering, I thought that human-based research would have even more regulatory bodies, thus would be less likely to be retracted because more people are ensuring the methods and results of the project. Additionally, working with people and human data can prove to be even more variated. Because of this, I wanted to be able to explore words related to human studies and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3214\n",
      "10619\n"
     ]
    }
   ],
   "source": [
    "list_articles = []\n",
    "irb = []\n",
    "\n",
    "#passing through each article\n",
    "for i in range(0, len(total['clean_text'])):\n",
    "    count = 0\n",
    "    word_count = 0\n",
    "    patient_count = 0\n",
    "    \n",
    "    #passing through each word in each article\n",
    "    for j in total['clean_text'][i].split():\n",
    "        \n",
    "        #human study/data terms\n",
    "        if j == 'IRB' or j == 'case' or j == 'participants':\n",
    "            \n",
    "            #comparing the word in the article to the list of human terms\n",
    "            if j =='IRB' or j == 'participants':\n",
    "                \n",
    "                #prevents the article from being repeatedly added to the lists\n",
    "                if i not in list_articles:\n",
    "                    list_articles.append(i)\n",
    "                    irb.append(1)\n",
    "                    count = 1\n",
    "            else:\n",
    "                #checks to see if the word after the occurrence of \"case\" is \"study\"\n",
    "                try:\n",
    "                    if total['clean_text'][i].split()[word_count+1] == 'study':\n",
    "                        \n",
    "                        #prevents the article from being repeatedly added to the lists\n",
    "                        if i not in list_articles:\n",
    "                            list_articles.append(i)\n",
    "                            irb.append(1)\n",
    "                            count = 1\n",
    "                    else:\n",
    "                        pass\n",
    "                \n",
    "                #if the next word is not \"study\", then the word does not count as a true occurrence\n",
    "                except:\n",
    "                    pass            \n",
    "        else:\n",
    "            pass\n",
    "        word_count += 1\n",
    "    if count == 0:\n",
    "        irb.append(0)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "#shows the number of articles that contained human terms\n",
    "print(len(list_articles))\n",
    "\n",
    "#shows if there will be a reshape error when adding to the dataframe\n",
    "print(len(irb))\n",
    "total['irb_binary'] = irb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words used to determine if an article uses human studies/data were \"IRB,\" \"case study,\" and \"participants.\" The above script was used to create a binary column, where the presence of one of these phrases was indicated by 1 while none of these phrases appearing in the article is indicated by 0.\n",
    "\n",
    "There was an approximately 15% decrease of human study/data terms in retracted articles compared to non-retracted articles. Thus, retracted articles were more likely to contain human study/data terms. Similarly to the animal studies information, it is possible that the variance in working with people or human data correlates to an article being retracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.829538\n",
       "1    0.170462\n",
       "Name: irb_binary, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retracted articles\n",
    "total['irb_binary'][:1537].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.674961\n",
       "1    0.325039\n",
       "Name: irb_binary, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#non-retracted articles\n",
    "total['irb_binary'][1537:].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regulatory Binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For research projects that have both animal studies and human studies, regulation is significantly more intense than only having one of those studies. Below is a script that combines the information of the \"irb_binary\" and \"animal_binary\" columns. If an article had both terms that relate to animal studies and human studies, then the value recorded is 2. If an article only had terms for one of those study types, then the value recorded is 1. Articles that make no mention of any terms related to animal or human studies are recorded as a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10619"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regulatory = []\n",
    "\n",
    "#passing through each article\n",
    "for i in range(0, len(total['irb_binary'])):\n",
    "    \n",
    "    #combining irb binary and animal binary information\n",
    "    if total['irb_binary'][i] == 1 or total['animal_binary'][i] == 1:\n",
    "        if total['irb_binary'][i] == 1 and total['animal_binary'][i] == 1:\n",
    "            regulatory.append(2)\n",
    "        else:\n",
    "            regulatory.append(1)\n",
    "    else:\n",
    "        regulatory.append(0)\n",
    "\n",
    "#shows if there will be a reshape error when adding to the dataframe\n",
    "len(regulatory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.545869\n",
       "0    0.384515\n",
       "2    0.069616\n",
       "Name: regulatory, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total['reg_binary'] = regulatory\n",
    "total = total.rename(columns={'reg_binary':'regulatory'})\n",
    "\n",
    "#retracted articles\n",
    "total['regulatory'][:1537].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.581150\n",
       "0    0.354327\n",
       "2    0.064523\n",
       "Name: regulatory, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#non-retracted articles\n",
    "total['regulatory'][1537:].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is very little difference between retracted and non-retracted articles for the regulatory binary value. The increase in regulation of having multiple regulated studies in one project does not seem to correlate to the article being retracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review articles are articles that look at other publish literature and discuss different aspects of this literature on a broader scale than individual research projects. Because of this, I thought that review articles may be more inclined to have issues with plagiarism. Often times, the introduction of these articles will say verbatim, \"in this review\" as one of the final lines to transition to the next section. The below script creates a binary column, where an article containing \"this review\" is indicated by 1 while an article that does not contain this phrase is indicated by 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234\n",
      "10619\n"
     ]
    }
   ],
   "source": [
    "list_articles = []\n",
    "review = []\n",
    "\n",
    "#passing through each article\n",
    "for i in range(0, len(total['clean_text'])):\n",
    "    count = 0\n",
    "    word_count = 0\n",
    "    \n",
    "    #passing through each word in each article\n",
    "    for j in total['clean_text'][i].split():\n",
    "        \n",
    "        #review term\n",
    "        if j == 'review':\n",
    "            \n",
    "            #checks to see if the word before the occurrence of \"review\" is \"this\"\n",
    "            if total['clean_text'][i].split()[word_count-1] == 'this':\n",
    "                \n",
    "                #prevents the article from being repeatedly added to the lists\n",
    "                if i not in list_articles:\n",
    "                    list_articles.append(i)\n",
    "                    review.append(1)\n",
    "                    count = 1\n",
    "        \n",
    "        #if the previous word is not \"this\", then the word does not count as a true occurrence\n",
    "        else:\n",
    "            pass\n",
    "        word_count += 1\n",
    "    \n",
    "    #used for if the article does not contain any review terms\n",
    "    if count == 0:\n",
    "        review.append(0)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "#shows the number of articles that contained review terms\n",
    "print(len(list_articles))\n",
    "\n",
    "#shows if there will be a reshape error when adding to the dataframe\n",
    "print(len(review))\n",
    "total['review_binary'] = review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an approximately 2% increase in the phrase \"this review\" in retracted articles compared to non-retracted articles. Thus, retracted articles are more likely to be review articles than non-retracted articles. While this percentage change is extremely small, it must be noted that the number of total occurrences of a review article is only 234. For a change to occur at all at this scale is important to pay attention to. It is possible that review papers correlate to the paper being retracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.960963\n",
       "1    0.039037\n",
       "Name: review_binary, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retracted articles\n",
    "total['review_binary'][:1537].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.980841\n",
       "1    0.019159\n",
       "Name: review_binary, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#non-retracted articles\n",
    "total['review_binary'][1537:].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Novel Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often times published literature will use the word \"novel\" to express that the ideas found within that article have never been published before. Research grants are often focused on funding new ideas and new ideas often lead to more citations. However, new ideas are also difficult to bring to fruition, as laying the groundwork of a completely new project requires a vast amount of troubleshooting. Because of this, I wanted to be able to explore words related to novel ideas. The below script creates a binary column, where an article containing the word \"novel\" more than once is indicated by 1 while an article not containing the word or only having one occurrence of the word is indicated by 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1565\n",
      "10619\n"
     ]
    }
   ],
   "source": [
    "list_articles = []\n",
    "novel_idea = []\n",
    "\n",
    "#passing through each article\n",
    "for i in range(0, len(total['clean_text'])):\n",
    "    count = 0\n",
    "    novel_count = 0\n",
    "    \n",
    "    #passing through each word in each article\n",
    "    for j in total['clean_text'][i].split():\n",
    "        \n",
    "        #novel term\n",
    "        if j == 'novel':\n",
    "            \n",
    "            #passing through each word again to see if it occurs a second time\n",
    "            for k in total['clean_text'][i].split():\n",
    "                if k == 'novel':\n",
    "                    novel_count += 1\n",
    "                    \n",
    "            #novel occurs more than once in the article\n",
    "            if novel_count > 1:\n",
    "                \n",
    "                #prevents the article from being repeatedly added to the lists\n",
    "                if i not in list_articles:\n",
    "                    list_articles.append(i)\n",
    "                    novel_idea.append(1)\n",
    "                    count = 1\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    #used for if the article does not contain the novel term\n",
    "    if count == 0:\n",
    "        novel_idea.append(0)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "#shows the number of articles that contained the novel term\n",
    "print(len(list_articles))\n",
    "\n",
    "#shows if there will be a reshape error when adding to the dataframe\n",
    "print(len(novel_idea))\n",
    "total['novel_idea'] = novel_idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is very little difference between retracted and non-retracted articles for the regulatory binary value. The project being a novel idea does not seem to correlate to the article being retracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.826936\n",
       "1    0.173064\n",
       "Name: novel_idea, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total['novel_idea'][:1537].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.85697\n",
       "1    0.14303\n",
       "Name: novel_idea, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total['novel_idea'][1537:].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text readability refers to how difficult a corpus is to read. Several measures for text readability exist, but often these measures use extremely similar equations or the measures use a reference list of words (with any word not being on that list increasing the readability score significantly). Because these articles are extremely jargon forward, I determined that the text readability measures I would use are the Flesch Reading Ease and Flesch Kincaid Grade Value, found within the textstat library. More information about these equations can be found at [source 1](https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests) and [source 2](https://pypi.org/project/textstat/).\n",
    "\n",
    "The below function determines the value of these measures for each text and then adds the value to a new column corresponding to each measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to determine readability measures for each article\n",
    "def readability(dataframe):\n",
    "    flesch_reading_ease_value = []\n",
    "    flesch_kincaid_grade_value = []\n",
    "    \n",
    "    for i in dataframe['clean_text']:\n",
    "        flesch_reading_ease_value.append(textstat.flesch_reading_ease(i))\n",
    "        flesch_kincaid_grade_value.append(textstat.flesch_kincaid_grade(i))\n",
    "\n",
    "    dataframe['flesch_reading_ease'] = flesch_reading_ease_value\n",
    "    dataframe['flesch_kincaid_grade'] = flesch_kincaid_grade_value\n",
    "    \n",
    "    #shows if there will be a reshape error when adding to the dataframe\n",
    "    print(len(dataframe['flesch_reading_ease']))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537\n",
      "9082\n",
      "10619\n"
     ]
    }
   ],
   "source": [
    "readability(retract)\n",
    "readability(no_retract)\n",
    "readability(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the feature engineered columns created for each dataframe were saved as new CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "retract.to_csv('./retract_feature_engineered_data.csv')\n",
    "no_retract.to_csv('./no_retract_feature_engineered_data.csv')\n",
    "total.to_csv('./total_feature_engineered_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
