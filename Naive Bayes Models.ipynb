{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_retract = pd.read_csv('./no_retraction_data_cleaned.csv')\n",
    "no_retract = no_retract.drop(columns=['Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1'])\n",
    "\n",
    "retract = pd.read_csv('./retraction_data_cleaned.csv')\n",
    "retract = retract.drop(columns=['Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>volume</th>\n",
       "      <th>issue</th>\n",
       "      <th>journal</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "      <th>publisher</th>\n",
       "      <th>retraction_binary</th>\n",
       "      <th>unpacked_keywords</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_text_lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1208/s12249-016-0596-x</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>AAPS PharmSciTech</td>\n",
       "      <td>Study of the Transformations of Micro/Nano-cry...</td>\n",
       "      <td>‘Polymorphism’ generally referred as the abili...</td>\n",
       "      <td>This study elucidates the physical properties ...</td>\n",
       "      <td>['monoclinic', 'nano-sized crystals', 'orthorh...</td>\n",
       "      <td>Springer International Publishing</td>\n",
       "      <td>1</td>\n",
       "      <td>['monoclinic', 'nano-sized', 'crystals', 'orth...</td>\n",
       "      <td>Polymorphism generally referred as the ability...</td>\n",
       "      <td>Polymorphism generally referred a the ability ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1021/acscentsci.9b00224</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>ACS central science</td>\n",
       "      <td>Targeted Protein Internalization and Degradati...</td>\n",
       "      <td>Traditional\\ndrug development efforts are focu...</td>\n",
       "      <td>Targeted</td>\n",
       "      <td>[]</td>\n",
       "      <td>American Chemical Society</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>Traditional drug development efforts are focus...</td>\n",
       "      <td>Traditional drug development effort are focuse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1021/acsomega.8b00488</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>ACS omega</td>\n",
       "      <td>Regulating the Microstructure of Intumescent F...</td>\n",
       "      <td>Intumescent flame retardants\\nare now being us...</td>\n",
       "      <td>A compatibilizer</td>\n",
       "      <td>[]</td>\n",
       "      <td>American Chemical Society</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>Intumescent flame retardants are now being use...</td>\n",
       "      <td>Intumescent flame retardant are now being used...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1021/acsomega.8b00153</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>ACS omega</td>\n",
       "      <td>Solid-to-Solid Crystallization of Organic Thin...</td>\n",
       "      <td>Crystal growth process is basic and essential ...</td>\n",
       "      <td>The solid-to-solid crystallization processes o...</td>\n",
       "      <td>[]</td>\n",
       "      <td>American Chemical Society</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>Crystal growth process is basic and essential ...</td>\n",
       "      <td>Crystal growth process is basic and essential ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1107/S1600536811022574</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acta crystallographica. Section E, Structure r...</td>\n",
       "      <td>Oxonium picrate.</td>\n",
       "      <td>For general background to organic salts of pic...</td>\n",
       "      <td>The title compound, H3O+·C6H2N3O7</td>\n",
       "      <td>[]</td>\n",
       "      <td>International Union of Crystallography</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>For general background to organic salts of pic...</td>\n",
       "      <td>For general background to organic salt of picr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          doi    year  month   day  volume  issue  \\\n",
       "0   10.1208/s12249-016-0596-x  2016.0    8.0  10.0    18.0    5.0   \n",
       "1  10.1021/acscentsci.9b00224  2019.0    5.0   9.0     5.0    6.0   \n",
       "2    10.1021/acsomega.8b00488  2018.0    6.0  27.0     3.0    6.0   \n",
       "3    10.1021/acsomega.8b00153  2018.0    6.0  25.0     3.0    6.0   \n",
       "4   10.1107/S1600536811022574  2011.0    6.0  18.0    67.0    NaN   \n",
       "\n",
       "                                             journal  \\\n",
       "0                                  AAPS PharmSciTech   \n",
       "1                                ACS central science   \n",
       "2                                          ACS omega   \n",
       "3                                          ACS omega   \n",
       "4  Acta crystallographica. Section E, Structure r...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Study of the Transformations of Micro/Nano-cry...   \n",
       "1  Targeted Protein Internalization and Degradati...   \n",
       "2  Regulating the Microstructure of Intumescent F...   \n",
       "3  Solid-to-Solid Crystallization of Organic Thin...   \n",
       "4                                   Oxonium picrate.   \n",
       "\n",
       "                                                text  \\\n",
       "0  ‘Polymorphism’ generally referred as the abili...   \n",
       "1  Traditional\\ndrug development efforts are focu...   \n",
       "2  Intumescent flame retardants\\nare now being us...   \n",
       "3  Crystal growth process is basic and essential ...   \n",
       "4  For general background to organic salts of pic...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  This study elucidates the physical properties ...   \n",
       "1                                           Targeted   \n",
       "2                                   A compatibilizer   \n",
       "3  The solid-to-solid crystallization processes o...   \n",
       "4                  The title compound, H3O+·C6H2N3O7   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  ['monoclinic', 'nano-sized crystals', 'orthorh...   \n",
       "1                                                 []   \n",
       "2                                                 []   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "\n",
       "                                publisher  retraction_binary  \\\n",
       "0       Springer International Publishing                  1   \n",
       "1               American Chemical Society                  1   \n",
       "2               American Chemical Society                  1   \n",
       "3               American Chemical Society                  1   \n",
       "4  International Union of Crystallography                  1   \n",
       "\n",
       "                                   unpacked_keywords  \\\n",
       "0  ['monoclinic', 'nano-sized', 'crystals', 'orth...   \n",
       "1                                                 []   \n",
       "2                                                 []   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  Polymorphism generally referred as the ability...   \n",
       "1  Traditional drug development efforts are focus...   \n",
       "2  Intumescent flame retardants are now being use...   \n",
       "3  Crystal growth process is basic and essential ...   \n",
       "4  For general background to organic salts of pic...   \n",
       "\n",
       "                                      clean_text_lem  \n",
       "0  Polymorphism generally referred a the ability ...  \n",
       "1  Traditional drug development effort are focuse...  \n",
       "2  Intumescent flame retardant are now being used...  \n",
       "3  Crystal growth process is basic and essential ...  \n",
       "4  For general background to organic salt of picr...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = pd.read_csv('./total_data_cleaned.csv')\n",
    "total = total.drop(columns=['Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1'])\n",
    "total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = 'word',\n",
    "                            tokenizer = None,\n",
    "                            preprocessor = None,\n",
    "                            stop_words = stopwords.words(\"english\"), #stopwords were removed\n",
    "                            max_features = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(retract['clean_text_lem'])\n",
    "vect = vectorizer.transform(retract['clean_text_lem'])\n",
    "vect_retract = pd.DataFrame(vect.toarray(), columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0d0</th>\n",
       "      <th>10a</th>\n",
       "      <th>10b</th>\n",
       "      <th>10e8</th>\n",
       "      <th>10x</th>\n",
       "      <th>11a</th>\n",
       "      <th>11m088</th>\n",
       "      <th>125a</th>\n",
       "      <th>125b</th>\n",
       "      <th>126a</th>\n",
       "      <th>...</th>\n",
       "      <th>μmol</th>\n",
       "      <th>νmax</th>\n",
       "      <th>χ2</th>\n",
       "      <th>χn</th>\n",
       "      <th>χγ</th>\n",
       "      <th>χζ</th>\n",
       "      <th>ψb</th>\n",
       "      <th>ψh</th>\n",
       "      <th>ψp</th>\n",
       "      <th>ϵ0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0d0  10a  10b  10e8  10x  11a  11m088  125a  125b  126a  ...  μmol  νmax  \\\n",
       "0    0    0    0     0    0    0       0     0     0     0  ...     0     0   \n",
       "1    0    0    0     0    0    0       0     0     0     0  ...     0     0   \n",
       "2    0    0    0     0    0    0       0     0     0     0  ...     0     0   \n",
       "3    0    0    0     0    0    0       0     0     0     0  ...     0     0   \n",
       "4    0    0    0     0    0    0       0     0     0     0  ...     0     0   \n",
       "\n",
       "   χ2  χn  χγ  χζ  ψb  ψh  ψp  ϵ0  \n",
       "0   0   0   0   0   0   0   0   0  \n",
       "1   0   0   0   0   0   0   0   0  \n",
       "2   0   0   0   0   0   0   0   0  \n",
       "3   0   0   0   0   0   0   0   0  \n",
       "4   0   0   0   0   0   0   0   0  \n",
       "\n",
       "[5 rows x 10000 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_retract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have equation characters, have numerical/letter combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['0d0', '10a', '10b', '10e8', '10x', '11a', '11m088', '125a', '125b',\n",
       "       '126a',\n",
       "       ...\n",
       "       'behavioural', 'behind', 'beijing', 'bela', 'belfast', 'belgium',\n",
       "       'belief', 'believe', 'believed', 'belong'],\n",
       "      dtype='object', length=1000)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_retract.columns[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tendency', 'tendon', 'tends', 'tenofovir', 'tensile', 'tension',\n",
       "       'term', 'termed', 'terminal', 'terminally',\n",
       "       ...\n",
       "       'μmol', 'νmax', 'χ2', 'χn', 'χγ', 'χζ', 'ψb', 'ψh', 'ψp', 'ϵ0'],\n",
       "      dtype='object', length=1000)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_retract.columns[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0d0</th>\n",
       "      <th>10a</th>\n",
       "      <th>10b</th>\n",
       "      <th>10e8</th>\n",
       "      <th>10x</th>\n",
       "      <th>11a</th>\n",
       "      <th>11m088</th>\n",
       "      <th>125a</th>\n",
       "      <th>125b</th>\n",
       "      <th>126a</th>\n",
       "      <th>...</th>\n",
       "      <th>μmol</th>\n",
       "      <th>νmax</th>\n",
       "      <th>χ2</th>\n",
       "      <th>χn</th>\n",
       "      <th>χγ</th>\n",
       "      <th>χζ</th>\n",
       "      <th>ψb</th>\n",
       "      <th>ψh</th>\n",
       "      <th>ψp</th>\n",
       "      <th>ϵ0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1535</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>108</td>\n",
       "      <td>295</td>\n",
       "      <td>127</td>\n",
       "      <td>112</td>\n",
       "      <td>687</td>\n",
       "      <td>138</td>\n",
       "      <td>238</td>\n",
       "      <td>125</td>\n",
       "      <td>107</td>\n",
       "      <td>195</td>\n",
       "      <td>...</td>\n",
       "      <td>369</td>\n",
       "      <td>378</td>\n",
       "      <td>836</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>196</td>\n",
       "      <td>1176</td>\n",
       "      <td>420</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1538 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0d0  10a  10b  10e8  10x  11a  11m088  125a  125b  126a  ...  μmol  \\\n",
       "0       0    0    0     0    0    0       0     0     0     0  ...     0   \n",
       "1       0    0    0     0    0    0       0     0     0     0  ...     0   \n",
       "2       0    0    0     0    0    0       0     0     0     0  ...     0   \n",
       "3       0    0    0     0    0    0       0     0     0     0  ...     0   \n",
       "4       0    0    0     0    0    0       0     0     0     0  ...     0   \n",
       "...   ...  ...  ...   ...  ...  ...     ...   ...   ...   ...  ...   ...   \n",
       "1533    0    0    0     0    0    0       0     0     0     0  ...     0   \n",
       "1534    0    0    0     0    0    0       0     0     0     0  ...     0   \n",
       "1535    0    0    0     0    0    0       0     0     0     0  ...     0   \n",
       "1536    0    0    0     0    0    0       0     0     0     0  ...     0   \n",
       "1537  108  295  127   112  687  138     238   125   107   195  ...   369   \n",
       "\n",
       "      νmax   χ2   χn   χγ   χζ   ψb    ψh   ψp   ϵ0  \n",
       "0        0    0    0    0    0    0     0    0    0  \n",
       "1        0    0    0    0    0    0     0    0    0  \n",
       "2        0    0    0    0    0    0     0    0    0  \n",
       "3        0    0    0    0    0    0     0    0    0  \n",
       "4        0    0    0    0    0    0     0    0    0  \n",
       "...    ...  ...  ...  ...  ...  ...   ...  ...  ...  \n",
       "1533     0    2    0    0    0    0     0    0    0  \n",
       "1534     0    0    0    0    0    0     0    0    0  \n",
       "1535     0    0    0    0    0    0     0    0    0  \n",
       "1536     0    8    0    0    0    0     0    0    0  \n",
       "1537   378  836  112  112  112  196  1176  420  198  \n",
       "\n",
       "[1538 rows x 10000 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = []\n",
    "for i in vect_retract.columns:\n",
    "    word_sum = vect_retract[i].sum()\n",
    "    ls.append(word_sum) #create a list of the total number of times each word was used\n",
    "\n",
    "#The script below was adapted from https://www.kite.com/python/answers/how-to-append-a-list-as-a-row-to-a-pandas-dataframe-in-python\n",
    "sum_row = pd.Series(ls, index = vect_retract.columns) #the list was turned into a series\n",
    "vect_retract = vect_retract.append(sum_row, ignore_index = True) #the series was added to the end of the dataframe as a new row\n",
    "vect_retract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wa               157152\n",
       "cell             128810\n",
       "study             42884\n",
       "group             40348\n",
       "using             39680\n",
       "level             39027\n",
       "patient           37508\n",
       "expression        35759\n",
       "figure            35648\n",
       "control           35270\n",
       "usepackage        33089\n",
       "used              31573\n",
       "protein           31496\n",
       "al                30500\n",
       "et                29665\n",
       "data              28523\n",
       "analysis          27986\n",
       "fig               27421\n",
       "gene              26404\n",
       "time              26017\n",
       "also              25222\n",
       "treatment         24966\n",
       "effect            24135\n",
       "ml                23430\n",
       "result            22334\n",
       "ha                21947\n",
       "antibody          21716\n",
       "two               21521\n",
       "vitamin           20472\n",
       "mouse             19963\n",
       "sample            19096\n",
       "compared          18733\n",
       "different         17579\n",
       "significant       17245\n",
       "value             17214\n",
       "concentration     17061\n",
       "one               17035\n",
       "model             17035\n",
       "well              17003\n",
       "day               16819\n",
       "high              16804\n",
       "disease           16737\n",
       "performed         16691\n",
       "shown             16551\n",
       "table             16511\n",
       "significantly     16392\n",
       "may               16379\n",
       "response          16061\n",
       "activity          15977\n",
       "mean              15827\n",
       "Name: 1537, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_retract.iloc[1537, :].sort_values(ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NO RETRACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(no_retract['clean_text_lem'])\n",
    "vect = vectorizer.transform(no_retract['clean_text_lem'])\n",
    "vect_no_retract = pd.DataFrame(vect.toarray(), columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_no_retract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_no_retract.columns[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_no_retract.columns[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "for i in vect_no_retract.columns:\n",
    "    word_sum = vect_no_retract[i].sum()\n",
    "    ls.append(word_sum) #create a list of the total number of times each word was used\n",
    "\n",
    "#The script below was adapted from https://www.kite.com/python/answers/how-to-append-a-list-as-a-row-to-a-pandas-dataframe-in-python\n",
    "sum_row = pd.Series(ls, index = vect_no_retract.columns) #the list was turned into a series\n",
    "vect_no_retract = vect_no_retract.append(sum_row, ignore_index = True) #the series was added to the end of the dataframe as a new row\n",
    "vect_no_retract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_no_retract.iloc[3427, :].sort_values(ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOTAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>volume</th>\n",
       "      <th>issue</th>\n",
       "      <th>journal</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "      <th>publisher</th>\n",
       "      <th>retraction_binary</th>\n",
       "      <th>unpacked_keywords</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_text_lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1208/s12249-016-0596-x</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>AAPS PharmSciTech</td>\n",
       "      <td>Study of the Transformations of Micro/Nano-cry...</td>\n",
       "      <td>‘Polymorphism’ generally referred as the abili...</td>\n",
       "      <td>This study elucidates the physical properties ...</td>\n",
       "      <td>['monoclinic', 'nano-sized crystals', 'orthorh...</td>\n",
       "      <td>Springer International Publishing</td>\n",
       "      <td>1</td>\n",
       "      <td>['monoclinic', 'nano-sized', 'crystals', 'orth...</td>\n",
       "      <td>Polymorphism generally referred as the ability...</td>\n",
       "      <td>Polymorphism generally referred a the ability ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1021/acscentsci.9b00224</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>ACS central science</td>\n",
       "      <td>Targeted Protein Internalization and Degradati...</td>\n",
       "      <td>Traditional\\ndrug development efforts are focu...</td>\n",
       "      <td>Targeted</td>\n",
       "      <td>[]</td>\n",
       "      <td>American Chemical Society</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>Traditional drug development efforts are focus...</td>\n",
       "      <td>Traditional drug development effort are focuse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1021/acsomega.8b00488</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>ACS omega</td>\n",
       "      <td>Regulating the Microstructure of Intumescent F...</td>\n",
       "      <td>Intumescent flame retardants\\nare now being us...</td>\n",
       "      <td>A compatibilizer</td>\n",
       "      <td>[]</td>\n",
       "      <td>American Chemical Society</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>Intumescent flame retardants are now being use...</td>\n",
       "      <td>Intumescent flame retardant are now being used...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1021/acsomega.8b00153</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>ACS omega</td>\n",
       "      <td>Solid-to-Solid Crystallization of Organic Thin...</td>\n",
       "      <td>Crystal growth process is basic and essential ...</td>\n",
       "      <td>The solid-to-solid crystallization processes o...</td>\n",
       "      <td>[]</td>\n",
       "      <td>American Chemical Society</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>Crystal growth process is basic and essential ...</td>\n",
       "      <td>Crystal growth process is basic and essential ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1107/S1600536811022574</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acta crystallographica. Section E, Structure r...</td>\n",
       "      <td>Oxonium picrate.</td>\n",
       "      <td>For general background to organic salts of pic...</td>\n",
       "      <td>The title compound, H3O+·C6H2N3O7</td>\n",
       "      <td>[]</td>\n",
       "      <td>International Union of Crystallography</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>For general background to organic salts of pic...</td>\n",
       "      <td>For general background to organic salt of picr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          doi    year  month   day  volume  issue  \\\n",
       "0   10.1208/s12249-016-0596-x  2016.0    8.0  10.0    18.0    5.0   \n",
       "1  10.1021/acscentsci.9b00224  2019.0    5.0   9.0     5.0    6.0   \n",
       "2    10.1021/acsomega.8b00488  2018.0    6.0  27.0     3.0    6.0   \n",
       "3    10.1021/acsomega.8b00153  2018.0    6.0  25.0     3.0    6.0   \n",
       "4   10.1107/S1600536811022574  2011.0    6.0  18.0    67.0    NaN   \n",
       "\n",
       "                                             journal  \\\n",
       "0                                  AAPS PharmSciTech   \n",
       "1                                ACS central science   \n",
       "2                                          ACS omega   \n",
       "3                                          ACS omega   \n",
       "4  Acta crystallographica. Section E, Structure r...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Study of the Transformations of Micro/Nano-cry...   \n",
       "1  Targeted Protein Internalization and Degradati...   \n",
       "2  Regulating the Microstructure of Intumescent F...   \n",
       "3  Solid-to-Solid Crystallization of Organic Thin...   \n",
       "4                                   Oxonium picrate.   \n",
       "\n",
       "                                                text  \\\n",
       "0  ‘Polymorphism’ generally referred as the abili...   \n",
       "1  Traditional\\ndrug development efforts are focu...   \n",
       "2  Intumescent flame retardants\\nare now being us...   \n",
       "3  Crystal growth process is basic and essential ...   \n",
       "4  For general background to organic salts of pic...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  This study elucidates the physical properties ...   \n",
       "1                                           Targeted   \n",
       "2                                   A compatibilizer   \n",
       "3  The solid-to-solid crystallization processes o...   \n",
       "4                  The title compound, H3O+·C6H2N3O7   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  ['monoclinic', 'nano-sized crystals', 'orthorh...   \n",
       "1                                                 []   \n",
       "2                                                 []   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "\n",
       "                                publisher  retraction_binary  \\\n",
       "0       Springer International Publishing                  1   \n",
       "1               American Chemical Society                  1   \n",
       "2               American Chemical Society                  1   \n",
       "3               American Chemical Society                  1   \n",
       "4  International Union of Crystallography                  1   \n",
       "\n",
       "                                   unpacked_keywords  \\\n",
       "0  ['monoclinic', 'nano-sized', 'crystals', 'orth...   \n",
       "1                                                 []   \n",
       "2                                                 []   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  Polymorphism generally referred as the ability...   \n",
       "1  Traditional drug development efforts are focus...   \n",
       "2  Intumescent flame retardants are now being use...   \n",
       "3  Crystal growth process is basic and essential ...   \n",
       "4  For general background to organic salts of pic...   \n",
       "\n",
       "                                      clean_text_lem  \n",
       "0  Polymorphism generally referred a the ability ...  \n",
       "1  Traditional drug development effort are focuse...  \n",
       "2  Intumescent flame retardant are now being used...  \n",
       "3  Crystal growth process is basic and essential ...  \n",
       "4  For general background to organic salt of picr...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = total['clean_text_lem']\n",
    "y = total['retraction_binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    69.06\n",
       "1    30.94\n",
       "Name: retraction_binary, dtype: float64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize=True).mul(100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = 'word',\n",
    "                            tokenizer = None,\n",
    "                            preprocessor = None,\n",
    "                            stop_words = stopwords.words(\"english\"), #stopwords were removed\n",
    "                            max_features = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(X_train)\n",
    "#stopwords not super helpful because always talking in third person passive\n",
    "#stopwords should also be lemmatized\n",
    "#need to make science stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(X_train.toarray(),\n",
    "                          columns=vectorizer.get_feature_names())\n",
    "X_test_df = pd.DataFrame(X_test.toarray(),\n",
    "                          columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.fit(X_train_df, y_train)\n",
    "print(mnb.score(X_train_df, y_train))\n",
    "print(mnb.score(X_test_df, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(mnb.coef_[0]))\n",
    "print(min(mnb.coef_[0]))\n",
    "print((mnb.coef_[0]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(mnb, X_test_df, y_test, cmap='Blues', values_format='d');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5000 features, default everything, 71.9% train, 69.5% test, 202 true/predict 1, 182 true 1/predict 0\n",
    "#10000 features, default everything, 75.0% train, 69.9% test, 198 true/predict 1, 186 true 1/predict 0\n",
    "#20000 features, default everything, 80.0% train, 70.6% test, 200 true/predict 1, 184 true 1/predict 0\n",
    "#50000 features, default everything, 85.7% train, 72.4% test, 195 true/predict 1, 189 true 1/predict 0\n",
    "#100000 features, default everything, 87.5% train, 72.6% test, 197 true/predict 1, 187 true 1/predict 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding to Stopwords List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lem_stopwords = [lemmatizer.lemmatize(i) for i in stopwords.words(\"english\")]\n",
    "lem_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retraction_stopwords = ['wa', 'using', 'et', 'al', 'figure', 'usepackage', 'used', 'fig', 'also', \n",
    "                        'ml', 'ha', 'two', 'one', 'may', 'based', 'table', 'however', 'data', 'mm', 'ms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.quora.com/Is-there-any-list-of-stopwords-related-to-scientific-papers\n",
    "science_stopwords = ['any','apply','applying','reapplying','given','papers','paper','about',\n",
    "                     'results','result','real','world','page','article','present','takes',\n",
    "                     'account', 'previous','work','propose','proposes','proposed','simply','simple',\n",
    "                     'demonstrate','demonstrated','demonstrates','realworld','datasets','dataset',\n",
    "                     'provide','important','research','researchers','experiments','experiment','unexpected',\n",
    "                     'discovering','using','recent','collected','solve','columns','existing','traditional',\n",
    "                     'final','consider','presented','provides','automatically','extracting','including','help',\n",
    "                     'helps','explore','illustrate','achieve','better']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = total['clean_text_lem']\n",
    "y = total['retraction_binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = 'word',\n",
    "                            tokenizer = None,\n",
    "                            preprocessor = None,\n",
    "                            stop_words = stopwords.words(\"english\")+ lem_stopwords + retraction_stopwords + science_stopwords, #stopwords were removed\n",
    "                            max_features = 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(X_train.toarray(),\n",
    "                          columns=vectorizer.get_feature_names())\n",
    "X_test_df = pd.DataFrame(X_test.toarray(),\n",
    "                          columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.fit(X_train_df, y_train)\n",
    "print(mnb.score(X_train_df, y_train))\n",
    "print(mnb.score(X_test_df, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(mnb.coef_[0]))\n",
    "print(min(mnb.coef_[0]))\n",
    "print((mnb.coef_[0]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(0, len(mnb.coef_[0])):\n",
    "    if mnb.coef_[0][i] == -15.715015446704813:\n",
    "        count +=1\n",
    "    else:\n",
    "        pass\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(mnb, X_test_df, y_test, cmap='Blues', values_format='d');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5000 features, more stopwords, 71.7% train, 69.3% test, 199 true/predict 1, 185 true 1/predict 0\n",
    "#20000 features, more stopwords, 80.1% train, 70.8% test, 200 true/predict 1, 184 true 1/predict 0\n",
    "#100000 features, more stopwords, 87.6% train, 72.6% test, 196 true/predict 1, 188 true 1/predict 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = 'word',\n",
    "                            tokenizer = None,\n",
    "                            preprocessor = None,\n",
    "                            stop_words = stopwords.words(\"english\")+ lem_stopwords + retraction_stopwords + science_stopwords, #stopwords were removed\n",
    "                            max_features = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(retract['clean_text_lem'])\n",
    "vect = vectorizer.transform(retract['clean_text_lem'])\n",
    "vect_retract = pd.DataFrame(vect.toarray(), columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_retract.columns[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_retract.columns[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "for i in vect_retract.columns:\n",
    "    word_sum = vect_retract[i].sum()\n",
    "    ls.append(word_sum) #create a list of the total number of times each word was used\n",
    "\n",
    "#The script below was adapted from https://www.kite.com/python/answers/how-to-append-a-list-as-a-row-to-a-pandas-dataframe-in-python\n",
    "sum_row = pd.Series(ls, index = vect_retract.columns) #the list was turned into a series\n",
    "vect_retract = vect_retract.append(sum_row, ignore_index = True) #the series was added to the end of the dataframe as a new row\n",
    "vect_retract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_retract.iloc[1537, :].sort_values(ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(no_retract['clean_text_lem'])\n",
    "vect = vectorizer.transform(no_retract['clean_text_lem'])\n",
    "vect_no_retract = pd.DataFrame(vect.toarray(), columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_no_retract.columns[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_no_retract.columns[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "for i in vect_no_retract.columns:\n",
    "    word_sum = vect_no_retract[i].sum()\n",
    "    ls.append(word_sum) #create a list of the total number of times each word was used\n",
    "\n",
    "#The script below was adapted from https://www.kite.com/python/answers/how-to-append-a-list-as-a-row-to-a-pandas-dataframe-in-python\n",
    "sum_row = pd.Series(ls, index = vect_no_retract.columns) #the list was turned into a series\n",
    "vect_no_retract = vect_no_retract.append(sum_row, ignore_index = True) #the series was added to the end of the dataframe as a new row\n",
    "vect_no_retract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_no_retract.iloc[3427, :].sort_values(ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = total['clean_text_lem']\n",
    "y = total['retraction_binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer(ngram_range=(1, 2), max_features=5000, \n",
    "                       stop_words = stopwords.words(\"english\")+ lem_stopwords + retraction_stopwords + science_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(tvec.fit_transform(X_train).toarray(),\n",
    "                  columns=tvec.get_feature_names())\n",
    "X_test_df = pd.DataFrame(tvec.transform(X_test).toarray(),\n",
    "                  columns=tvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.fit(X_train_df, y_train)\n",
    "print(mnb.score(X_train_df, y_train))\n",
    "print(mnb.score(X_test_df, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(mnb, X_test_df, y_test, cmap='Blues', values_format='d');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5000, more stopwords/tfidf bigrams, 75.2% train, 73.2% test, 183 predict/true 1, 201 predict 0/true 1\n",
    "#20000, more stopwords/tfidf bigrams, 78.5% train, 74.5% test, 135 predict/true 1, 249 predict 0/true 1\n",
    "#100000, more stopwords/tfidf bigrams, 75.7% train, 73.2% test, 57 predict/true 1, 327 predict 0/true 1\n",
    "\n",
    "#5000, more stopwords/tfidf trigrams, 75.1% train, 73.2% test, 182 predict/true 1, 202 predict 0/true 1\n",
    "#20000, more stopwords/tfidf trigrams, 78.2% train, 74.2% test, 135 predict/true 1, 249 predict 0/true 1\n",
    "#100000, more stopwords/tfidf trigrams, 75.8% train, 73.8% test, 67 predict/true 1, 317 predict 0/true 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looked at top 30 for \"5000, more stopwords/tfidf bigrams, 75.2% train, 73.2% test, 183 predict/true 1, 201 predict 0/true 1\": all were single words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing Test Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = total['clean_text_lem']\n",
    "y = total['retraction_binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.2,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = 'word',\n",
    "                            tokenizer = None,\n",
    "                            preprocessor = None,\n",
    "                            stop_words = stopwords.words(\"english\")+ lem_stopwords + retraction_stopwords + science_stopwords, #stopwords were removed\n",
    "                            max_features = 20000)\n",
    "\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train.toarray(),\n",
    "                          columns=vectorizer.get_feature_names())\n",
    "X_test_df = pd.DataFrame(X_test.toarray(),\n",
    "                          columns=vectorizer.get_feature_names())\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(X_train_df, y_train)\n",
    "print(mnb.score(X_train_df, y_train))\n",
    "print(mnb.score(X_test_df, y_test))\n",
    "\n",
    "plot_confusion_matrix(mnb, X_test_df, y_test, cmap='Blues', values_format='d');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer(ngram_range=(1, 2), max_features=5000, \n",
    "                       stop_words = stopwords.words(\"english\")+ lem_stopwords + retraction_stopwords + science_stopwords)\n",
    "\n",
    "X_train_df = pd.DataFrame(tvec.fit_transform(X_train).toarray(),\n",
    "                  columns=tvec.get_feature_names())\n",
    "X_test_df = pd.DataFrame(tvec.transform(X_test).toarray(),\n",
    "                  columns=tvec.get_feature_names())\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(X_train_df, y_train)\n",
    "print(mnb.score(X_train_df, y_train))\n",
    "print(mnb.score(X_test_df, y_test))\n",
    "\n",
    "plot_confusion_matrix(mnb, X_test_df, y_test, cmap='Blues', values_format='d');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46905537459283386\n",
      "0.5208333333333334\n",
      "0.5140997830802603\n",
      " \n",
      "0.46905537459283386\n",
      "0.4765625\n",
      "0.4750542299349241\n"
     ]
    }
   ],
   "source": [
    "#20000 features, more stopwords/test size 0.2, 79.7% train, 70.7% test, 144 true/predict 1, 163 true 1/predict 0\n",
    "print(144/(144+163))\n",
    "#20000 features, more stopwords, 80.1% train, 70.8% test, 200 true/predict 1, 184 true 1/predict 0\n",
    "print(200/(200+184))\n",
    "#20000 features, more stopwords/test size 0.3, 81.1% train, 71.6% test, 237 true/predict 1, 224 true 1/predict 0\n",
    "print(237/(237+224))\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "#5000, more stopwords/tfidf bigrams/test size 0.2, 74.9% train, 73.3% test, 144 predict/true 1, 163 predict 0/true 1\n",
    "print(144/(144+163))\n",
    "#5000, more stopwords/tfidf bigrams, 75.2% train, 73.2% test, 183 predict/true 1, 201 predict 0/true 1\n",
    "print(183/(183+201))\n",
    "#5000, more stopwords/tfidf bigrams/test size 0.3, 75.1% train, 73.8% test, 219 predict/true 1, 242 predict 0/true 1\n",
    "print(219/(219+242))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
