{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biopython (labeled as Bio) is a free Python library developed for work in bioinformatics. Within the biopython library is Entrez Programming Utilities (labeled as Entrez): a set of scripts that serve as a way to query any databases that fall under the National Center for Biotechnology Information. These databases include both PubMed and PMC (the two databases used for this project). For more information about these libraries and functions, please see the README file in this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of the difficulties with pulling the data necessary for this project is that published academic research is typically only accessible if a fee is paid. To avoid this problem, the corpi of articles for this project needed to be able to be accessed completely from the PMC database, a completely free, full-text database archive housed as a part of the PubMed database. Additionally, certain search terms were necessary to differentiate between retracted articles and non-retracted articles. However, these necessary search terms were not usable when querying the PMC database.\n",
    "\n",
    "Because of these complications, a several step process was used to be able to properly pull the necessary data. For the retraction data, a CSV file made available by PMC was used to determine all of the journals that can be found within the PMC archive. These journal names were included as part of the search terms for querying the PubMed database as well as the terms necessary for limiting the search to retracted articles. Information about the articles that were from the provided journal names and had also been retracted was placed into a dataframe, including the digital object identifier (DOI) of the article. This DOI is a unique string of characters that can be used to identify any published journal article. These DOI values were then used as the search terms for querying the PMC database to receive the complete corpus of each article that had been retracted.\n",
    "\n",
    "For the no retraction data, it was determined after initial research of the project that the journal PLOS ONE was responsible for a significant portion of the retracted journal articles. Additionally, this journal is not limited in scope when publishing journal articles, meaning that any topic in STEM is likely to be accepted by the journal. Because of this, it was determined that all non-retracted journal articles would be pulled from PLOS ONE to remove influences from querying on the metadata of the non-retracted articles. Thus, the PLOS ONE identifier was used as a search term for querying the PubMed database as well as a randomly assigned date. Similarly to the retraction data workflow, information about the articles was placed into a dataframe, including the DOI of each article. The DOI values were then used as the search terms for querying the PMC database to receive the complete corpus of each PLOS ONE article that had not been retracted.\n",
    "\n",
    "All of this information was saved into various CSV files throughout the querying process to be cleaned in a later notebook. For more information about the difference between the PubMed and PMC database, please see the README file in this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Journal Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in the \"Data Collection Workflow\" section above, the journals that were part of the PMC database needed to be determined. The CSV file below contained this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./journals_in_pmc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Full              3104\n",
       " NIH Portfolio      448\n",
       "Name: Participation level, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Participation level'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all of the journals within the PMC database participated fully in releasing the text of each article. Those journals that did not participate fully needed to be removed so that querying time would not be wasted on articles that I would not be able to later access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat((df, pd.get_dummies(df['Participation level'], prefix='participation')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['participation_full'] = df['participation_ Full ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['participation_ NIH Portfolio ', 'participation_ Full '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([  17,   23,   24,   25,   26,   27,   28,   29,   30,   35,\n",
       "            ...\n",
       "            3396, 3408, 3415, 3427, 3456, 3464, 3476, 3478, 3479, 3544],\n",
       "           dtype='int64', length=448)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['participation_full'] == 0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.loc[df['participation_full'] == 0].index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./journals_in_pmc_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Journal title</th>\n",
       "      <th>NLM TA</th>\n",
       "      <th>pISSN</th>\n",
       "      <th>eISSN</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>LOCATORplus ID</th>\n",
       "      <th>Latest issue</th>\n",
       "      <th>Earliest volume</th>\n",
       "      <th>Free access</th>\n",
       "      <th>Open access</th>\n",
       "      <th>Participation level</th>\n",
       "      <th>Deposit status</th>\n",
       "      <th>Journal URL</th>\n",
       "      <th>participation_full</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3 Biotech</td>\n",
       "      <td>3 Biotech</td>\n",
       "      <td>2190-572X</td>\n",
       "      <td>2190-5738</td>\n",
       "      <td>Springer</td>\n",
       "      <td>101565857</td>\n",
       "      <td>v.10(9);Sep 2020</td>\n",
       "      <td>v.1;2011</td>\n",
       "      <td>12 months</td>\n",
       "      <td>Some</td>\n",
       "      <td>Full</td>\n",
       "      <td></td>\n",
       "      <td>http://www.ncbi.nlm.nih.gov/pmc/journals/1811/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3D Printing in Medicine</td>\n",
       "      <td>3D Print Med</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2365-6271</td>\n",
       "      <td>BioMed Central</td>\n",
       "      <td>101721758</td>\n",
       "      <td>v.5;Dec 2019</td>\n",
       "      <td>v.2;2016</td>\n",
       "      <td>Immediate</td>\n",
       "      <td>All</td>\n",
       "      <td>Full</td>\n",
       "      <td></td>\n",
       "      <td>http://www.ncbi.nlm.nih.gov/pmc/journals/3516/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AACE Clinical Case Reports</td>\n",
       "      <td>AACE Clin Case Rep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2376-0605</td>\n",
       "      <td>American Association of Clinical Endocrinologists</td>\n",
       "      <td>101670593</td>\n",
       "      <td>v.6(5);Sep-Oct 2020</td>\n",
       "      <td>v.5;2019</td>\n",
       "      <td>Immediate</td>\n",
       "      <td>No</td>\n",
       "      <td>Full</td>\n",
       "      <td></td>\n",
       "      <td>http://www.ncbi.nlm.nih.gov/pmc/journals/3582/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The AAPS Journal</td>\n",
       "      <td>AAPS J</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1550-7416</td>\n",
       "      <td>American Association of Pharmaceutical Scientists</td>\n",
       "      <td>101223209</td>\n",
       "      <td>v.18(3);May 2016</td>\n",
       "      <td>v.6;2004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Some</td>\n",
       "      <td>Full</td>\n",
       "      <td>No New Content</td>\n",
       "      <td>http://www.ncbi.nlm.nih.gov/pmc/journals/792/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPS PharmSci</td>\n",
       "      <td>AAPS PharmSci</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1522-1059</td>\n",
       "      <td>American Association of Pharmaceutical Scientists</td>\n",
       "      <td>100897065</td>\n",
       "      <td>v.6(2);Jun 2004</td>\n",
       "      <td>v.1;1999</td>\n",
       "      <td>Immediate</td>\n",
       "      <td>No</td>\n",
       "      <td>Full</td>\n",
       "      <td>Predecessor</td>\n",
       "      <td>http://www.ncbi.nlm.nih.gov/pmc/journals/989/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Journal title              NLM TA      pISSN      eISSN  \\\n",
       "0                   3 Biotech           3 Biotech  2190-572X  2190-5738   \n",
       "1     3D Printing in Medicine        3D Print Med        NaN  2365-6271   \n",
       "2  AACE Clinical Case Reports  AACE Clin Case Rep        NaN  2376-0605   \n",
       "3            The AAPS Journal              AAPS J        NaN  1550-7416   \n",
       "4               AAPS PharmSci       AAPS PharmSci        NaN  1522-1059   \n",
       "\n",
       "                                           Publisher LOCATORplus ID  \\\n",
       "0                                           Springer      101565857   \n",
       "1                                     BioMed Central      101721758   \n",
       "2  American Association of Clinical Endocrinologists      101670593   \n",
       "3  American Association of Pharmaceutical Scientists      101223209   \n",
       "4  American Association of Pharmaceutical Scientists      100897065   \n",
       "\n",
       "          Latest issue Earliest volume Free access Open access  \\\n",
       "0     v.10(9);Sep 2020        v.1;2011   12 months        Some   \n",
       "1         v.5;Dec 2019        v.2;2016   Immediate         All   \n",
       "2  v.6(5);Sep-Oct 2020        v.5;2019   Immediate          No   \n",
       "3     v.18(3);May 2016        v.6;2004         NaN        Some   \n",
       "4      v.6(2);Jun 2004        v.1;1999   Immediate          No   \n",
       "\n",
       "  Participation level    Deposit status  \\\n",
       "0               Full                      \n",
       "1               Full                      \n",
       "2               Full                      \n",
       "3               Full    No New Content    \n",
       "4               Full       Predecessor    \n",
       "\n",
       "                                      Journal URL  participation_full  \n",
       "0  http://www.ncbi.nlm.nih.gov/pmc/journals/1811/                   1  \n",
       "1  http://www.ncbi.nlm.nih.gov/pmc/journals/3516/                   1  \n",
       "2  http://www.ncbi.nlm.nih.gov/pmc/journals/3582/                   1  \n",
       "3   http://www.ncbi.nlm.nih.gov/pmc/journals/792/                   1  \n",
       "4   http://www.ncbi.nlm.nih.gov/pmc/journals/989/                   1  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./journals_in_pmc_clean.csv', index_col=False)\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to formatting issues within the CSV file, I had to thoroughly clean the CSV file to remove the journals that did not have full participation with the PMC archive. Once the appropriate journals had been removed, I saved dataframe as a new CSV file. The clean dataframe can be seen above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling Retractioned Articles Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrez has several methods that can be called for querying of the PubMed and PMC databases. This [article](https://medium.com/@kliang933/scraping-big-data-from-public-research-repositories-e-g-pubmed-arxiv-2-488666f6f29b) was used as a resource for understanding how to properly utilize these functions for the querying calls that were necessary for data collection. As described in the \"Data Collection Workflow\" section above, information obtained from PubMed queries must be pulled to continue the workflow process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling Data from PubMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lists that will become columns in the dataframe\n",
    "ls_num = []\n",
    "ls_id = []\n",
    "ls_doi = []\n",
    "ls_language = []\n",
    "ls_year = []\n",
    "ls_month = []\n",
    "ls_day = []\n",
    "ls_volume = []\n",
    "ls_issue = []\n",
    "ls_journal = []\n",
    "ls_title = []\n",
    "ls_page = []\n",
    "\n",
    "#setting up counter to keep track of how many articles had been pulled from each journal\n",
    "x=0\n",
    "\n",
    "#setting up counter to keep track of the number of articles that did not have a DOI\n",
    "no_doi = 0\n",
    "\n",
    "#to query retracted articles for each journal\n",
    "for i in range(0,len(df['NLM TA'])):\n",
    "    \n",
    "    #providing information for the query to be accepted\n",
    "    Entrez.email = 'my_email_address'\n",
    "    \n",
    "    #providing search terms for querying the PubMed database\n",
    "    handle = Entrez.esearch(db='pubmed',term='(hasretractionin) AND ('+df['NLM TA'][i]+'[Journal])', retmode='xml', retmax=1000)\n",
    "    \n",
    "    #formatting the results of the query: article IDs within the database\n",
    "    results = Entrez.read(handle)\n",
    "    ids = ' , '.join(results['IdList']) #joining the list of IDs together to be read in one large string\n",
    "    print(df['NLM TA'][i],[i]) #printing the name of the journal and the index number of the journal\n",
    "    \n",
    "    \n",
    "    #conditional in case there were no articles that met the search term criteria\n",
    "    if len(ids)==0:\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #providing information for the query to be accepted\n",
    "        Entrez.email = 'my_email_address'\n",
    "        \n",
    "        #providing IDs for querying\n",
    "        handle = Entrez.efetch(db='pubmed', id = ids, retmode='xml', rettype='full')\n",
    "        \n",
    "        #formatting the results of the query: information about the article\n",
    "        results_id = Entrez.read(handle)\n",
    "        \n",
    "        #counter to keep track of how many articles had been pulled from each journal\n",
    "        x += len(results['IdList'])\n",
    "        print(x)\n",
    "        \n",
    "        \n",
    "        #to sort through the infomration about each article pulled from each journal\n",
    "        for j in range(0, len(results['IdList'])):\n",
    "            \n",
    "            #adding the ID of each article to a list for the dataframe\n",
    "            ls_id.append(results['IdList'][j])\n",
    "            \n",
    "            #each piece of information has a try/except statement in case the specific piece of information\n",
    "            #is not given when the article is pulled\n",
    "            #if the try statement runs, then the piece of information will be added to a list\n",
    "            #if the try statement does not run, the a None value will be added to the list for that piece of information \n",
    "            \n",
    "            #extracting DOI of each article\n",
    "            try:\n",
    "                doi = str(results_id['PubmedArticle'][j]['MedlineCitation']['Article']['ELocationID'][0])\n",
    "                ls_doi.append(doi)\n",
    "            except:\n",
    "                ls_doi.append(None)\n",
    "                no_doi += 1\n",
    "                print(no_doi) #printing how many times the article pulled does not have a DOI\n",
    "                \n",
    "            #extracting the language used in each article\n",
    "            try:\n",
    "                language = str(results_id['PubmedArticle'][j]['MedlineCitation']['Article']['Language'][0])\n",
    "                ls_language.append(language)\n",
    "            except:\n",
    "                ls_language.append(None)\n",
    "                \n",
    "            #extracting the year each article was published  \n",
    "            try:\n",
    "                year = int(results_id['PubmedArticle'][j]['MedlineCitation']['Article']['ArticleDate'][0]['Year'])\n",
    "                ls_year.append(year)\n",
    "            except:\n",
    "                ls_year.append(None)\n",
    "            \n",
    "            #extracting the month each article was published\n",
    "            try:\n",
    "                month = int(results_id['PubmedArticle'][j]['MedlineCitation']['Article']['ArticleDate'][0]['Month'])\n",
    "                ls_month.append(month)\n",
    "            except:\n",
    "                ls_month.append(None)\n",
    "            \n",
    "            #extracting the day each article was published\n",
    "            try:\n",
    "                day = int(results_id['PubmedArticle'][j]['MedlineCitation']['Article']['ArticleDate'][0]['Day'])\n",
    "                ls_day.append(day)\n",
    "            except:\n",
    "                ls_day.append(None)\n",
    "            \n",
    "            #extracting the volume number of the journal when each article was published\n",
    "            try:\n",
    "                volume = int(results_id['PubmedArticle'][j]['MedlineCitation']['Article']['Journal']['JournalIssue']['Volume'])\n",
    "                ls_volume.append(volume)\n",
    "            except:\n",
    "                ls_volume.append(None)\n",
    "            \n",
    "            #extracting the issue number of the journal when each article was published\n",
    "            try:\n",
    "                issue = int(results_id['PubmedArticle'][j]['MedlineCitation']['Article']['Journal']['JournalIssue']['Issue'])\n",
    "                ls_issue.append(issue)\n",
    "            except:\n",
    "                ls_issue.append(None)\n",
    "            \n",
    "            #extracting the journal name where each article was published\n",
    "            try:\n",
    "                journal = str(results_id['PubmedArticle'][j]['MedlineCitation']['Article']['Journal']['Title'])\n",
    "                ls_journal.append(journal)\n",
    "            except:\n",
    "                ls_journal.append(None)\n",
    "            \n",
    "            #extracting the title of each article\n",
    "            try:\n",
    "                title = str(results_id['PubmedArticle'][j]['MedlineCitation']['Article']['ArticleTitle'])\n",
    "                ls_title.append(title)\n",
    "            except:\n",
    "                ls_title.append(None)\n",
    "            \n",
    "            #extracting the pagination for each article (or page numbers)\n",
    "            try:\n",
    "                page = str(results_id['PubmedArticle'][j]['MedlineCitation']['Article']['Pagination']['MedlinePgn'])\n",
    "                ls_page.append(page)\n",
    "            except:\n",
    "                ls_page.append(None)\n",
    "    \n",
    "    #added a sleep function to prevent 404 errors when pulling the data\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize the above script, each journal name is added to the other search terms necessary to pull retracted articles from the specified journal. The first query returns the ID number of each article (a specific identifier for PubMed) found. This ID number is then used to pull a large body of text that contains information about the article. Important pieces of information are extracted from this text and added to lists. If the information is not present, the script moves onto the next piece of information. The script prints the journal name, the index number of the journal in the \"journals_in_pmc_clean.csv\" file, the number of articles that each journal has that have been retracted, and the number of articles that have no DOI associated with them. These print statements allow me to watch the progress of the query because of its intricacy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([pd.Series(ls_id), pd.Series(ls_doi), pd.Series(ls_language), pd.Series(ls_year), pd.Series(ls_month), pd.Series(ls_day), pd.Series(ls_volume), pd.Series(ls_issue), pd.Series(ls_journal), pd.Series(ls_title), pd.Series(ls_page)], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('./pubmed_data_retraction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information lists created were concatenated together to form a dataframe. Thus, the dataframe contained information about the PubMed ID number, DOI, language, year, month, day, volume, issue, journal name, title, and page numbers for each article that had been retracted from the journals provided in the \"journals_in_pmc_clean.csv\" file. The dataframe was then saved to a CSV file as a proofing measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./pubmed_data_retraction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(axis=0, subset=['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eng    2139\n",
       "chi       2\n",
       "spa       1\n",
       "fre       1\n",
       "Name: 2, dtype: int64"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([2968, 2969], dtype='int64')\n",
      "Int64Index([259], dtype='int64')\n",
      "Int64Index([2243], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "print(data.loc[data['2']=='chi'].index)\n",
    "print(data.loc[data['2']=='spa'].index)\n",
    "print(data.loc[data['2']=='fre'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eng    2139\n",
       "Name: 2, dtype: int64"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop([2968, 2969, 259, 2243], axis=0)\n",
    "data['2'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe that contained the information about each retracted article needed to be cleaned. Articles that had no DOI could not be queried for in the PMC database to receive the full text of the article. Because of this, articles without a DOI were dropped from the dataframe. I would not be able to read articles that were not written in English (as it is the only language that I can fluently read). Because of this, any article that wasn't written in English were dropped from the dataframe as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_index = []\n",
    "for i in data['1']:\n",
    "    if len(i) < 7:\n",
    "        for j in range(0,len(data.loc[data['1']==i].index)):\n",
    "            ls_index.append(data.loc[data['1']==i].index[j])\n",
    "data = data.drop(ls_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the articles had DOI values that were extremely small. These DOI values showed in intial project research to be problematic in pulling from the PMC database. Because of this, any DOI value that was less than 7 characters was dropped from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('./datasets/pubmed_data_retraction_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./datasets/pubmed_data_retraction_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicates in the dataframe were dropped. The now clean dataframe was saved to a CSV file as a proofing measure for the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212             10.1186/1471-2121-13-8\n",
       "618           10.3389/fpsyg.2016.01298\n",
       "1877      10.1371/journal.pone.0001444\n",
       "895            10.1074/jbc.M111.260414\n",
       "1439             10.1093/neuonc/nor116\n",
       "1995        10.1038/s41598-019-38519-5\n",
       "602           10.3389/fphar.2017.00871\n",
       "511                10.7554/eLife.12248\n",
       "2108           10.4103/2229-5070.72109\n",
       "1953         10.1186/s12978-019-0732-7\n",
       "1175    10.1523/JNEUROSCI.2613-09.2009\n",
       "1428                10.1038/ncomms6446\n",
       "98             10.4103/0256-4947.83211\n",
       "1432                10.1038/ncomms1623\n",
       "44           10.1107/S160053680706254X\n",
       "599            10.3389/fonc.2013.00153\n",
       "1488               10.2147/OTT.S124118\n",
       "1            10.1208/s12249-016-0596-x\n",
       "899            10.1074/jbc.M111.247726\n",
       "2002        10.1038/s41598-017-10365-3\n",
       "111            10.1074/jbc.M111.329078\n",
       "1598      10.1371/journal.pone.0218664\n",
       "1392              10.1128/MCB.01480-09\n",
       "1340     10.1158/1535-7163.MCT-14-0672\n",
       "1384              10.1128/MCB.00114-14\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['1'].sample(n=25,replace = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly selected samples were pulled from the DOI column of the dataframe. These DOIs were used to check manually that the articles from the query were indeed articles that had been retracted. All the DOIs shown above were proven to be for retracted articles, so it was assumed that the query was successful and the workflow of data collection could proceed to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling Data from PMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lists that will become columns in the dataframe\n",
    "ls_total_text = []\n",
    "ls_total_keywords = []\n",
    "ls_total_abstract = []\n",
    "ls_publisher = []\n",
    "\n",
    "#setting up a counter to keep track of the index number of the DOI being pulled\n",
    "count = 2075\n",
    "\n",
    "#setting up a counter to keep track of the number of articles that did not have any text\n",
    "no_text = 0\n",
    "\n",
    "#to query a specific number of DOIs of retracted articles\n",
    "for i in data['1'][2075:2106]:\n",
    "    \n",
    "    #providing information for the query to be accepted\n",
    "    Entrez.email = 'my_email_address'\n",
    "    \n",
    "    #providing search terms for querying the PMC database\n",
    "    handle = Entrez.esearch(db='pmc',term=i, retmode='xml')\n",
    "    \n",
    "    #formatting the results of the query: article IDs within the database\n",
    "    results = Entrez.read(handle)\n",
    "    ids = ' , '.join(results['IdList'])\n",
    "    \n",
    "    #providing information for the query to be accepted\n",
    "    Entrez.email = 'my_email_address'\n",
    "    \n",
    "    #providing IDs for querying\n",
    "    handle = Entrez.efetch(db='pmc', id = ids, retmode='xml', rettype='full')\n",
    "    \n",
    "    #formatting the results of the query: information about the article\n",
    "    text = handle.read()\n",
    "\n",
    "    #using BeautifulSoup to pull specific information from the article\n",
    "    soup = BeautifulSoup(text, \"lxml\")\n",
    "    \n",
    "    #count to keep track of which DOI is currently being processed\n",
    "    print(f\"{i} [{count}]\")\n",
    "    \n",
    "    #each piece of information has a try/except statement in case the specific piece of information\n",
    "    #is not given when the article is pulled\n",
    "    #if the try statement runs, then the piece of information will pulled from the html and added to a list\n",
    "    #if the try statement does not run, the a None value will be added to the list for that piece of information\n",
    "    \n",
    "    #extracting full text for each article\n",
    "    try:\n",
    "        #list to hold each section of each article\n",
    "        ls_raw_text = []\n",
    "        \n",
    "        #string value that will hold the complete text\n",
    "        text = str()\n",
    "        \n",
    "        #finding each section of each article\n",
    "        for i in range(0,len(soup.find_all(\"sec\"))):\n",
    "            for j in range(0,len(soup.find_all(\"sec\")[i].find_all(\"p\"))): #indexing through each paragraph in each section\n",
    "                ls_raw_text.append(str(soup.find_all(\"sec\")[i].find_all(\"p\")[j].text))\n",
    "        \n",
    "        #combining the different sections of the article into one large body of text\n",
    "        for i in range(0,len(ls_raw_text)):\n",
    "            text += ls_raw_text[i]\n",
    "        ls_total_text.append(text)\n",
    "        \n",
    "        #count to keep track of the number of articles that had no text\n",
    "        if text=='':\n",
    "            no_text += 1\n",
    "            print(f'No text --> {no_text}')\n",
    "    except:\n",
    "        ls_total_text.append(None)\n",
    "    \n",
    "    #extracting keywords list for each article\n",
    "    try:\n",
    "        ls_keywords = []\n",
    "        for i in range(0,len(soup.find_all(\"kwd\"))):\n",
    "            ls_keywords.append(soup.find_all(\"kwd\")[i].text)\n",
    "        ls_total_keywords.append(ls_keywords)\n",
    "    except:\n",
    "        ls_total_keywords.append(None)\n",
    "    \n",
    "    #extracting the abstract for each article\n",
    "    try:\n",
    "        ls_abstract = []\n",
    "        for i in soup.find_all(\"abstract\")[0].text.split('\\n'): #splitting on \\n due to formatting from html\n",
    "            if i == '':\n",
    "                pass\n",
    "            else:\n",
    "                ls_abstract.append(i)\n",
    "        ls_total_abstract.append(ls_abstract[0])\n",
    "    except:\n",
    "        ls_total_abstract.append(None)\n",
    "    \n",
    "    #extracting the publisher name for each article\n",
    "    try:      \n",
    "        ls_publisher.append(soup.find_all(\"publisher-name\")[0].text)\n",
    "    except:\n",
    "        ls_publisher.append(None)\n",
    "    \n",
    "    count +=1\n",
    "    \n",
    "    #added a sleep function to prevent 404 errors when pulling the data\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize the above script, each DOI for each retracted article is used as a search term for querying the PMC database. The resulting ID value of the article (as the ID value is different between the PMC and PubMed databases for each article) is pulled and then used to query the PMC database for information about the article. This method pulls the full text, keywords list, abstract, and publisher for each retracted article. As the script runs, the current DOI, index number of the DOI, and a count of how many articles have no text are printed to allow me to keep track of when the function ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(data['1'][2075:2106]).to_csv('./doi__2075_2106.csv')\n",
    "pd.Series(ls_total_text).to_csv('./text__2075_2106.csv')\n",
    "pd.Series(ls_total_keywords).to_csv('./keywords__2075_2106.csv')\n",
    "pd.Series(ls_total_abstract).to_csv('./abstract__2075_2106.csv')\n",
    "pd.Series(ls_publisher).to_csv('./publisher__2075_2106.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, the script would run through the entire DOI column. However, when using this function, I often had 404 errors or the script would stop abruptly with no error message. For this reason, I decided to break the DOI column into small sections to closely monitor the information and save the information I received frequently. Because of this, several CSV files were created, each labeled with what DOI index values were used when pulling the information. This information was then cleaned into one complete CSV file in a notebook that will not be provided. The complete CSV file is used for the \"Data Cleaning\" notebook instead of the large number of individual CSV files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling Non-Retracted Articles Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in the \"Data Collection Workflow\" section above, information obtained from PubMed queries must be pulled to continue the workflow process. In this instance, the querying will be specifically for articles from the PLOS ONE journal that were not retracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling Data from PubMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16',\n",
    "                 '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31']\n",
    "len(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "start_month = ['01/01', '02/01', '03/01', '04/01', '05/01', '06/01', \n",
    "               '07/01', '08/01', '09/01', '10/01', '11/01', '12/01']\n",
    "end_month = ['01/31', '02/28', '03/31', '04/30', '05/31', '06/30', \n",
    "            '07/31', '08/31', '09/30', '10/31', '11/30', '12/31']\n",
    "\n",
    "month = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "year = ['2015', '2016', '2017', '2018', '2019']\n",
    "start_date = []\n",
    "end_date = []\n",
    "\n",
    "#determining a random date for each month between 2015-2019\n",
    "for i in year:\n",
    "    for j in range(0,len(month)):\n",
    "        \n",
    "        #for each month that has 31 days\n",
    "        if j == '01' or j == '03' or j == '05' or j == '07' or j == '08' or j == '10' or j == '12':\n",
    "            day_num = np.random.choice(day, 1)\n",
    "            \n",
    "        #for february\n",
    "        elif j == '02':\n",
    "            day_num = np.random.choice(day[:28])\n",
    "            \n",
    "        #for each month that has 30 days\n",
    "        else:\n",
    "            day_num = np.random.choice(day[:30])\n",
    "        start_date.append(i+'/'+start_month[j])\n",
    "        end_date.append(i+'/'+month[j]+'/'+day_num)\n",
    "        \n",
    "print(len(start_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the last five years of PLOS ONE volumes, there were approximately 150000 articles that were published. To try not bias the date in which the article was published, a random date was determined for each month between 2015-2019. The above script determined this list of dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as this script is nearly identical to an earlier script, please check previous script\n",
    "#for additional control flow comments\n",
    "\n",
    "ls_id = []\n",
    "ls_doi = []\n",
    "ls_language = []\n",
    "ls_year = []\n",
    "ls_month = []\n",
    "ls_day = []\n",
    "ls_volume = []\n",
    "ls_issue = []\n",
    "ls_journal = []\n",
    "ls_title = []\n",
    "ls_page = []\n",
    "x=0\n",
    "no_doi = 0\n",
    "\n",
    "for i in range(0,len(start_date)):\n",
    "    Entrez.email = 'my_email_address'\n",
    "    handle = Entrez.esearch(db='pubmed',term='(PLoS One [Journal])', retmode='xml', retmax=167, mindate = start_date[i], maxdate = end_date[i])\n",
    "    results = Entrez.read(handle)\n",
    "    \n",
    "    ids = ' , '.join(results['IdList'])\n",
    "\n",
    "    \n",
    "    if len(ids)==0:\n",
    "        pass\n",
    "    else:\n",
    "        Entrez.email = 'my_email_address'\n",
    "        handle = Entrez.efetch(db='pubmed', id = ids, retmode='xml', rettype='full')\n",
    "        results_id = Entrez.read(handle)\n",
    "        \n",
    "        x += len(results['IdList'])\n",
    "        print(x)\n",
    "        \n",
    "        for j in range(0, len(results['IdList'])):\n",
    "            ls_id.append(results['IdList'][j])\n",
    "            try:\n",
    "                doi = str(results_id['PubmedArticle'][j]['MedlineCitation']['Article']['ELocationID'][0])\n",
    "                ls_doi.append(doi)\n",
    "            except:\n",
    "                ls_doi.append(None)\n",
    "                no_doi += 1\n",
    "                print(no_doi)\n",
    "                \n",
    "            try:\n",
    "                language = str(results_id['PubmedArticle'][j]['MedlineCitation']['Article']['Language'][0])\n",
    "                ls_language.append(language)\n",
    "            except:\n",
    "                ls_language.append(None)\n",
    "                \n",
    "            try:\n",
    "                year = int(results_id['PubmedArticle'][j]['MedlineCitation']['Article']['ArticleDate'][0]['Year'])\n",
    "                ls_year.append(year)\n",
    "                print(year)\n",
    "            except:\n",
    "                ls_year.append(None)\n",
    "            \n",
    "            try:\n",
    "                month = int(results_id['PubmedArticle'][j]['MedlineCitation']['Article']['ArticleDate'][0]['Month'])\n",
    "                ls_month.append(month)\n",
    "                print(month)\n",
    "            except:\n",
    "                ls_month.append(None)\n",
    "                \n",
    "            try:\n",
    "                day = int(results_id['PubmedArticle'][j]['MedlineCitation']['Article']['ArticleDate'][0]['Day'])\n",
    "                ls_day.append(day)\n",
    "                print(day)\n",
    "            except:\n",
    "                ls_day.append(None)\n",
    "                \n",
    "            try:\n",
    "                volume = int(results_id['PubmedArticle'][j]['MedlineCitation']['Article']['Journal']['JournalIssue']['Volume'])\n",
    "                ls_volume.append(volume)\n",
    "            except:\n",
    "                ls_volume.append(None)\n",
    "            \n",
    "            try:\n",
    "                issue = int(results_id['PubmedArticle'][j]['MedlineCitation']['Article']['Journal']['JournalIssue']['Issue'])\n",
    "                ls_issue.append(issue)\n",
    "            except:\n",
    "                ls_issue.append(None)\n",
    "                \n",
    "            try:\n",
    "                journal = str(results_id['PubmedArticle'][j]['MedlineCitation']['Article']['Journal']['Title'])\n",
    "                ls_journal.append(journal)\n",
    "            except:\n",
    "                ls_journal.append(None)\n",
    "                \n",
    "            try:\n",
    "                title = str(results_id['PubmedArticle'][j]['MedlineCitation']['Article']['ArticleTitle'])\n",
    "                ls_title.append(title)\n",
    "            except:\n",
    "                ls_title.append(None)\n",
    "                \n",
    "            try:\n",
    "                page = str(results_id['PubmedArticle'][j]['MedlineCitation']['Article']['Pagination']['MedlinePgn'])\n",
    "                ls_page.append(page)\n",
    "            except:\n",
    "                ls_page.append(None)\n",
    "        \n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize the above script, each journal name is added to the other search terms necessary to pull non-retracted articles from PLOS ONE between 2015-2019. The first query returns the ID number of each article found. This ID number is then used to pull a large body of text that contains information about the article. Important pieces of information are extracted from this text and added to lists. If the information is not present, the script moves onto the next piece of information. The script prints the number of articles that have been determined. This print statement allows me to watch the progress of the query because of its intricacy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_retract = pd.concat([pd.Series(ls_id), pd.Series(ls_doi), pd.Series(ls_language), pd.Series(ls_year), pd.Series(ls_month), pd.Series(ls_day), pd.Series(ls_volume), pd.Series(ls_issue), pd.Series(ls_journal), pd.Series(ls_title), pd.Series(ls_page)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2019    1983\n",
       "2016    1893\n",
       "2018    1890\n",
       "2015    1880\n",
       "2017    1843\n",
       "2014     124\n",
       "Name: 3, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_no_retract[3].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12    959\n",
       "5     930\n",
       "10    918\n",
       "7     835\n",
       "9     835\n",
       "3     823\n",
       "2     804\n",
       "1     768\n",
       "11    743\n",
       "8     724\n",
       "6     668\n",
       "4     606\n",
       "Name: 4, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_no_retract[4].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14    806\n",
       "12    642\n",
       "13    490\n",
       "23    477\n",
       "31    474\n",
       "15    463\n",
       "28    424\n",
       "22    421\n",
       "11    420\n",
       "6     385\n",
       "8     334\n",
       "7     324\n",
       "18    317\n",
       "24    310\n",
       "5     303\n",
       "10    301\n",
       "19    269\n",
       "1     253\n",
       "9     243\n",
       "16    238\n",
       "3     236\n",
       "25    219\n",
       "27    212\n",
       "17    199\n",
       "4     172\n",
       "2     169\n",
       "26    167\n",
       "20    156\n",
       "21     98\n",
       "29     91\n",
       "Name: 5, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_no_retract[5].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information lists created were concatenated together to form a dataframe. Thus, the dataframe contained information about the PubMed ID number, DOI, language, year, month, day, volume, issue, journal name, title, and page numbers for each article that had not been retracted from PLOS ONE. The dataframe was then saved to a CSV file as a proofing measure. The distribution of the year, month, and day values was checked to ensure that the date had been randomized properly when querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_retract.to_csv('./pubmed_data_second_no_retraction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_retract = pd.read_csv('./pubmed_data_second_no_retraction.csv')\n",
    "data_no_retract = data_no_retract.dropna(axis=0, subset=['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eng    9613\n",
       "Name: 2, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_no_retract['2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_index = []\n",
    "for i in data_no_retract['1']:\n",
    "    if len(i) < 7:\n",
    "        for j in range(0,len(data_no_retract.loc[data_no_retract['1']==i].index)):\n",
    "            ls_index.append(data_no_retract.loc[data_no_retract['1']==i].index[j])\n",
    "data_no_retract = data_no_retract.drop(ls_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_retract = data_no_retract.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe that contained the information about each non-retracted article needed to be cleaned. Articles that had no DOI could not be queried for in the PMC database to receive the full text of the article. Because of this, articles without a DOI were dropped from the dataframe. All of the articles pulled were written in English, thus no articles needed to be dropped. Some of the articles had DOI values that were extremely small. These DOI values showed in intial project research to be problematic in pulling from the PMC database. Because of this, any DOI value that was less than 7 characters was dropped from the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling Data from PMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as this script is nearly identical to an earlier script, please check previous script\n",
    "#for additional control flow comments\n",
    "\n",
    "ls_total_text = []\n",
    "ls_total_keywords = []\n",
    "ls_total_abstract = []\n",
    "ls_publisher = []\n",
    "count = 6166\n",
    "no_text = 0\n",
    "\n",
    "for i in data_no_retract['1'][6166:]:\n",
    "    Entrez.email = 'my_email_address'\n",
    "    handle = Entrez.esearch(db='pmc',term=i, retmode='xml')\n",
    "    results = Entrez.read(handle)\n",
    "\n",
    "    ids = ' , '.join(results['IdList'])\n",
    "    Entrez.email = 'my_email_address'\n",
    "    handle = Entrez.efetch(db='pmc', id = ids, retmode='xml', rettype='full')\n",
    "    text = handle.read()\n",
    "\n",
    "    soup = BeautifulSoup(text, \"lxml\")\n",
    "    print(f\"{i} [{count}]\")\n",
    "    \n",
    "    try:\n",
    "        ls_raw_text = []\n",
    "        text = str()\n",
    "        for i in range(0,len(soup.find_all(\"sec\"))):\n",
    "            for j in range(0,len(soup.find_all(\"sec\")[i].find_all(\"p\"))):\n",
    "                ls_raw_text.append(str(soup.find_all(\"sec\")[i].find_all(\"p\")[j].text))\n",
    "\n",
    "        for i in range(0,len(ls_raw_text)):\n",
    "            text += ls_raw_text[i]\n",
    "        ls_total_text.append(text)\n",
    "        \n",
    "        if text=='':\n",
    "            no_text += 1\n",
    "            print(f'No text --> {no_text}')\n",
    "    except:\n",
    "        ls_total_text.append(None)\n",
    "    \n",
    "    try:\n",
    "        ls_keywords = []\n",
    "        for i in range(0,len(soup.find_all(\"kwd\"))):\n",
    "            ls_keywords.append(soup.find_all(\"kwd\")[i].text)\n",
    "        ls_total_keywords.append(ls_keywords)\n",
    "    except:\n",
    "        ls_total_keywords.append(None)\n",
    "    \n",
    "    try:\n",
    "        ls_abstract = []\n",
    "        for i in soup.find_all(\"abstract\")[0].text.split('\\n'):\n",
    "            if i == '':\n",
    "                pass\n",
    "            else:\n",
    "                ls_abstract.append(i)\n",
    "        ls_total_abstract.append(ls_abstract[0])\n",
    "    except:\n",
    "        ls_total_abstract.append(None)\n",
    "    \n",
    "    try:      \n",
    "        ls_publisher.append(soup.find_all(\"publisher-name\")[0].text)\n",
    "    except:\n",
    "        ls_publisher.append(None)\n",
    "    \n",
    "    count +=1\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(data_no_retract['1'][6166:]).to_csv('./doi__no_retraction__6166_end.csv')\n",
    "pd.Series(ls_total_text).to_csv('./text__no_retraction__6166_end.csv')\n",
    "pd.Series(ls_total_keywords).to_csv('./keywords__no_retraction__6166_end.csv')\n",
    "pd.Series(ls_total_abstract).to_csv('./abstract__no_retraction__6166_end.csv')\n",
    "pd.Series(ls_publisher).to_csv('./publisher__no_retraction__6166_end.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize the above script, each DOI for each non-retracted article is used as a search term for querying the PMC database. The resulting ID value of the article (as the ID value is different between the PMC and PubMed databases for each article) is pulled and then used to query the PMC database for information about the article. This method pulls the full text, keywords list, abstract, and publisher for each non-retracted article from PLOS ONE. As the script runs, the current DOI, index number of the DOI, and a count of how many articles have no text are printed to allow me to keep track of when the function ends.\n",
    "\n",
    "Similarly to when pulling the retracted articles, the script would ideally have run through the entire DOI column. However, when using this function, I often had 404 errors or the script would stop abruptly with no error message. For this reason, I decided to break the DOI column into three different sections to closely monitor the information and save the information I received more frequently. Because of this, three different CSV files were created, each labeled with what DOI index values were used when pulling the information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
